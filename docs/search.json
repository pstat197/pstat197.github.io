[
  {
    "objectID": "about/links.html",
    "href": "about/links.html",
    "title": "Links",
    "section": "",
    "text": "Attendance reporting form; fill out once per class meeting.\nCapstone project intake form; fill out by September 23."
  },
  {
    "objectID": "about/links.html#resources",
    "href": "about/links.html#resources",
    "title": "Links",
    "section": "Resources",
    "text": "Resources\nTextbooks:\n\nModern Data Science with R by Baumer, Kaplan, and Horton.\nIntroduction to Statistical Learning with Applications in R by James et al.\nFundamentals of Data Visualization by Claus Wilke.\nR for Data Science by Wickham and Grolemund.\n\nDocumentation:\n\nTidyverse and tidymodels packages\nGitHub Docs"
  },
  {
    "objectID": "about/outcomes.html",
    "href": "about/outcomes.html",
    "title": "Learning outcomes",
    "section": "",
    "text": "using modern technology and version control to collaborate efficiently on programming for data science projects;\nrecognizing and articulating problem patterns based on data semantics and one or more research questions;\nidentifying and accessing resources to aid in learning independently about methodology and/or application domains pertinent to a problem of interest;\ncommunicating data analysis and/or research findings in a project team setting and to a small audience of peers.\n\nCourse staff are committed to creating an inclusive learning environment. Data science involves a combination of computing, statistics and probability, and domain expertise, as well as use of technology and narrative communication and storytelling, and no one person should expect to be an expert in all of these areas. Course staff recognize this fact that core competencies vary considerably, acknowledge that each student has particular strengths and weaknesses and interests, and make their best effort to avoid promoting one skill set over others in the practice of data science."
  },
  {
    "objectID": "about/schedule.html",
    "href": "about/schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "Week\nTheme\nTuesday meeting\nThursday meeting\nSection meeting\n\n\n\n\n0\nModule 0: Introductions\nNO CLASS\nCourse orientation\nNO LAB\n\n\n1\nModule 0: Introductions\n0.1 Lecture:\n\non research projects in(volving) data science\n\n0.2 Activity:\n\ncollaboration using GitHub\n\nSoftware and technology overview\n\n\n2\nModule 0: Introductions\n0.3 Lecture/discussion:\n\nintroducing class survey data\n\n0.4 Activity:\n\nexploratory and descriptive analysis\n\ntidyverse\n\n\n3\nModule 1: biomarkers\n1.1 Discussion/lecture:\n\nsharing results of survey data analysis;\nintroducing biomarker data\n\n1.2 Lecture:\n\non prediction\n\ntidymodels\n\n\n4\nModule 1: biomarkers\n1.3 Lecture:\n\non classification\n\n1.4 Lecture/discussion:\n\non variable selection;\nreview published analysis of biomarker data\n\nclassification\n\n\n5\nModule 2: web fraud\n2.1 Lecture/discussion:\n\nsharing analysis of soil temperature data;\nintroducing web fraud data\n\n2.2 Lecture:\n\non text as data\n\ntext processing\n\n\n6\nModule 2: web fraud\n2.3 Lecture:\n\non multiclass classification\n\n2.4 Activity:\n\nmeasuring classification accuracy\n\nkeras\n\n\n7\nModule 3: soil temperature\n3.1 Discussion/lecture:\n\nsharing results of biomarker analysis;\nintroducing soil temperature data\n\n3.2 Lecture:\n\non time\n\ntime series analysis\n\n\n8\nModule 3: soil temperature\n3.3 Lecture:\n\non space\n\n3.4 Discussion: results\nspatial analysis\n\n\n9\nModule 4: vignettes\n4.1 Activity:\n\nworkshopping vignettes\n\nNO CLASS\nNO LAB\n\n\n10\nModule 4: vignettes\n4.2 Activity:\n\nteaching exchange\n\n4.3 Activity/discussion:\n\nteaching exchange;\nclosing\n\nNO LAB"
  },
  {
    "objectID": "about/syllabus.html",
    "href": "about/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Concurrent course listing: PSTAT197A and CMPSC190DD are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Introduction to research skills. Discussion of current research trends, writing literature reviews, etc. Students will be required to present materials reflecting their interests, which will be critically appraised for both content and presentation. Emphasis will be placed on aiding students to acquire a high-level of professionalism. Prerequisite: PSTAT126."
  },
  {
    "objectID": "about/syllabus.html#meetings",
    "href": "about/syllabus.html#meetings",
    "title": "Course syllabus",
    "section": "Meetings",
    "text": "Meetings\nClass meetings are held 2pm – 3:15pm Tuesdays and Thursdays in Ellison 2617.\nSection meetings are held on Wednesdays:\n\n2pm – 2:50pm in Girvetz 2115 with Josh;\n3pm – 3:50pm in Girvetz 2116 with Erika;\n4pm – 4:50pm in North Hall 1109 with Megan."
  },
  {
    "objectID": "about/syllabus.html#staff",
    "href": "about/syllabus.html#staff",
    "title": "Course syllabus",
    "section": "Staff",
    "text": "Staff\nInstructor:\n\nTrevor Ruiz. Visiting assistant professor and co-instructor for 2021-2022 capstone projects.\n\nTeaching assistants:\n\nErika McPhillips. MS/PhD student and capstone project mentor in 2021-2022.\nJoshua Bang. MS/PhD student and capstone project mentor in 2021-2022.\nMeghan Elcheikhali. PhD student and capstone project mentor in 2021-2022.\n\nUndergraduate learning assistant:\n\nYan Lashchev. BS student and Data Science Fellow at UCSB in 2021-2022."
  },
  {
    "objectID": "about/syllabus.html#expectations-and-assessments",
    "href": "about/syllabus.html#expectations-and-assessments",
    "title": "Course syllabus",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\nMuch of the course is designed around group activity and discussion. Students are therefore expected to:\n\nprepare for class meetings in advance by completing any assigned reading or activity;\nattend and actively participate in class meetings and section meetings;\nprovide meaningful, timely, and concrete contributions to group activities.\n\nStudents having any difficulty in meeting these expectations should raise the issue(s) promptly with the instructor.\nQualitative feedback is emphasized over numerical scores. Students are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview."
  },
  {
    "objectID": "about/syllabus.html#policies",
    "href": "about/syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance is expected. Each student can miss two sessions without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review posted session notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may not be accepted.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and quarantine. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies."
  },
  {
    "objectID": "about/technology.html",
    "href": "about/technology.html",
    "title": "Technology",
    "section": "",
    "text": "Computing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR version 4.2.0+\nRStudio version 2022.07.1+\nGit version 2.36.1+\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files."
  },
  {
    "objectID": "about/technology.html#sec-github",
    "href": "about/technology.html#sec-github",
    "title": "Technology",
    "section": "GitHub",
    "text": "GitHub\nStudents will learn and practice basic functionality of Git and GitHub for version control and collaboration by accessing course materials via GitHub repositories and submitting work via repository contributions.\nWe have a GitHub classroom for the data science capstone. Materials will be deployed via direct links. Students will be asked to submit work by contributing to team repositories; any such contributions will remain visible to course staff and team contributors, and so are not strictly private.\nTo access GitHub Classroom materials students will need to create a GitHub account if they do not already have one. Here is some advice on choosing a username."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science capstone preparation",
    "section": "",
    "text": "Most students take this course to prepare for their work on sponsored team projects during the remainder of the capstone sequence (PSTAT197B-C/CMPSC190DE-DF). However, some may elect to take the course for other reasons, such as an upcoming faculty-supervised research project or internship.\nThis site hosts course information and resources for currently enrolled students."
  },
  {
    "objectID": "materials/activities/github-basics.html",
    "href": "materials/activities/github-basics.html",
    "title": "GitHub basics",
    "section": "",
    "text": "This activity introduces GitHub repositories and basic Git actions; students will be expected to use these skills to access materials and complete assignments.\nObjectives:\nPrerequisites: completion of lab 1, particularly cloning the group sandbox repository."
  },
  {
    "objectID": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "href": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "title": "GitHub basics",
    "section": "Why are we using Git and GitHub?",
    "text": "Why are we using Git and GitHub?\nVersion control has many benefits, including the ability to track changes and contributions precisely, work in parallel with other contributors, revert to prior versions of files, keep track of issues, quickly share and disseminate work, and solicit user contributions from the coding public. Arguably, for all of these reasons and because of its widespread use, Git/GitHub is a must for data scientists.\nIn this class you’ll learn and practice some basics that will allow you to easily access course files, collaborate with each other, and efficiently submit your coursework. This should equip you to utilize a repository for efficient collaboration with your peers on your capstone project."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-workflow",
    "href": "materials/activities/github-basics.html#basic-workflow",
    "title": "GitHub basics",
    "section": "Basic workflow",
    "text": "Basic workflow\nIf I am working out of a repository and want to alter a file and make those changes available to anyone else accessing my repository, most of the time I need to:\n\ncreate/update local copies of repository files on my laptop;\nmake the desired change(s) locally;\nsend the changes back to the remote repository.\n\nTypically these steps are performed iteratively as work progresses – they are a basic workflow.\nWorkflow can be understood as a sequence of Git actions: actions that modify the repository files and/or metadata. The most basic sequence that accomplishes the above steps is:\n\ngit pull update the local repository (technically, fetch changes + merge changes from the remote repository);\ngit add stage file changes to be committed to the local repository;\ngit commit commit staged changes to the local repository;\ngit push send committed changes back to the remote repository.\n\nSometimes contributors take different or additional actions; the complexity of the Git actions required to make a change depends largely on repository settings, permissions, and agreements among collaborators about how workflow should be structured."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-git-actions",
    "href": "materials/activities/github-basics.html#basic-git-actions",
    "title": "GitHub basics",
    "section": "Basic Git actions",
    "text": "Basic Git actions\nHere you’ll make a local change and then push that change to the remote repository.\n\nPull\nThe first step to making a change is ensuring you have the most up-to-date version of the repository files.\n\n\n\n\n\n\nAction (individual)\n\n\n\nPull changes from the remote repository.\n\nIn your GitHub client, open the group sandbox repository and then look for a ‘Pull’ menu item.\nIf you are using GitHub desktop, you can alternatively ‘fetch origin’ first via a toolbar button. This will retrieve changes but without modifying local files, and if changes are detected, a button will appear in the main screen of the client to pull changes.\nIn the terminal: navigate to the root directory of the repository and git pull\n\n\n\nNow check the repository history to see what changes you just pulled. In GitHub Desktop, there is a history tab on the left-hand side that lists commits chronologically. Select a commit to view line-by-line differences for every file that was altered.\nYou should see two changes: that there is now a class-activity folder containing a copy of this activity; and the README file has been updated. Look at the differences on the readme file.\n\n\n\n\n\n\nRemark\n\n\n\nFetching vs. pulling\nFetching allows you to retrieve changes from the remote repository without merging them into your local repository. If there are commits that you haven’t merged, you can examine them before doing so in one of two ways:\n\nin the terminal, git diff main origin/main\nopen the remote repository on github.com and check the commit history (look for a clock icon with the number of commits in the upper right corner of the file navigator in the code menu); open any commit to see a line-by-line comparison of differences.\n\n\n\n\n\nMake changes\nNow that you have the most up-to-date version of all files, create a new markdown file in the class activity folder with a fun fact about you (or anything else if you’d rather) that you’ll upload to the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a markdown file:\n\nIn RStudio, select File > New File > Markdown File\nAdd an ‘About Me’ or similar header (use one or more hashes # before the header text)\nWrite a fun fact about yourself\nSave the file as YOURGITHUBUSERNAME-about.md in the class activity folder\n\n\n\n\n\nStage and commit changes\nNow that your new file is ready to go you can stage the changes to be committed to the repository and create a commit.\nA commit is a bundle of changes that will be submitted to the repository along with a message briefly explaining the changes made. Your GitHub client will often fill in a default message such as ‘update FILENAME.EXT’.\n\n\n\n\n\n\nAction (individual)\n\n\n\nStage and commit:\n\nIn your client, look for a menu item to add or stage changes. By default any changes made to any file will be included. In GitHub Desktop, look for the ‘Changes’ menu next to ‘History’; you can stage changes by simply selecting or unselecting the checkbox next to each file that was altered.\n\nOr in the terminal: git add FILENAME\n\nOnce you have staged changes, look for a menu item to commit changes. Add a message and commit the changes. In GitHub Desktop, this appears at the bottom of the ‘Changes’ menu.\n\nOr in the terminal: git add -m \"your message here\"\n\n\n\n\nOften these actions are performed together. However, in some workflows it may make sense to stage changes incrementally and create commits that bundle several changes at once. For example, if you need to make an update that requires modifying files A, B, and C, it may make sense to edit and stage changes to A first, followed by B, followed by C, and create the commit only once the full update has been implemented.\n\n\nPush\nThe last step is to push your commit to the remote repository. However, as you will see in a moment, too many people trying to push changes at once can create some problems.\n\n\n\n\n\n\nAction (group)\n\n\n\n\nChoose one person at your table to push their changes. The very first person to do this will have no problems, since their local repository is up to date with the remote.\nThen choose someone else to try – use the main screen at your workstation if possible so everyone at the table can see. Since the first person modified the remote repository, the next person to push changes will no longer be up to date. Git will detect this and the push won’t go through.\nHave the second person update their local repository by pulling changes, and then try the push again. It should go through once their local is up to date with the remote.\nHave everyone at your table pull changes but do not push any additional commits.\n\n\n\nSo far everyone is working on independent files and there’s no overlap between changes, so although it would be a bit of a hassle to have everyone check for changes every time they push, in principle it could be done. However, there is a more efficient way to work in parallel: by creating branches."
  },
  {
    "objectID": "materials/activities/github-basics.html#branching",
    "href": "materials/activities/github-basics.html#branching",
    "title": "GitHub basics",
    "section": "Branching",
    "text": "Branching\nInspect your GitHub client closely, and note that you are currently on the ‘main’ branch of the repository. Think of this as the primary version of the repository. Branches allow contributors to create parallel versions of the repository that they can modify for development purposes while leaving the primary version unaffected.\n\nCreate a branch\nHere you’ll use branches to avoid stepping on each others’ toes while pushing your table’s remaining commits. The strategy will be to create a personal branch, push your commit to that branch, and then merge the branch back into the main branch of the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a branch and push your previous commit:\n\nIn your GitHub client, look for a menu item to create and switch to a new branch.\nName your branch your GitHub username.\nCheck to see that you are currently on your personal branch.\nPush your previous commit. You shouldn’t have to repeat any of the previous steps, but you can if need be.\n\nIf you were one of the two who pushed their commit to main, make some small change to your file to push to your personal branch.\n\n\n\n\nAccess your neighbor’s branch\nWhile often the main purpose of branching is to create a version of the repository that only you will modify, contributors can inspect any branch of the repository. This can be useful for sharing ideas or getting input or help.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nMake a commit to your neighbor’s branch\n\nFind out your neighbor’s username and switch to their branch in your GitHub client.\nIn RStudio, verify that you are on their branch by executing git status in the terminal.\nOpen their markdown file, ask them a simple question about themselves (nothing too personal, please), and add the information to their markdown file.\nStage, commit, and push the change.\nWhen your neighbor has done the same with you, switch back to your own branch in your GitHub client and pull changes."
  },
  {
    "objectID": "materials/activities/github-basics.html#pull-requests",
    "href": "materials/activities/github-basics.html#pull-requests",
    "title": "GitHub basics",
    "section": "Pull requests",
    "text": "Pull requests\nOnce you are ready to integrate changes you’ve developed on a branch you can open a pull request to merge the development branch with the main branch. (Technically, pull requests can be opened between any two branches, so could also be used, for example, to update your branch if the main branch has new commits.)\n‘Pull request’ is a bit of an odd term; think of it as you making a request that your collaborators pull your changes for review.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nOpen a pull request:\n\nIn your GitHub client, find a menu item for opening a pull request. GitHub Desktop will simply redirect you to github.com to open the request.\nSpecify the pull request from your branch to the main branch and submit.\n\n\n\nOnce a pull request is opened, usually a collaborator with maintain privileges must be the one to merge changes and close the request. However, the rules for this depend on repository settings. For this repository, all contributors can merge and close pull requests.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\n\nOpen the repository in the browser. Navigate to pull requests.\nFind your neighbor’s pull request; merge their changes and close the request. Then delete the branch.\n\n\n\nOnce everyone at the table is finished, examine the repository on the main screen and verify that everyone’s markdown file is present on the main branch. Then have each contributor pull changes and check that they see the same."
  },
  {
    "objectID": "materials/activities/github-basics.html#merge-conflicts",
    "href": "materials/activities/github-basics.html#merge-conflicts",
    "title": "GitHub basics",
    "section": "Merge conflicts",
    "text": "Merge conflicts\nGit is pretty clever at merging changes when you pull, push, or merge branches via pull request. However, occasionally commits will conflict in such a way that can’t be resolved automatically. These are known as merge conflicts.\nMerge conflicts happen when:\n\ntwo commits differ on the same line of the same file;\nfiles are moved or deleted in conflicting ways.\n\nHere you’ll create an artificial merge conflict to see what this looks like and how to fix it.\n\n\n\n\n\n\nAction (group)\n\n\n\nCreate a merge conflict\nEnsure the workstation at your table is up to date with the remote repository. Then:\n\nHave someone at your table open the README file and add the group members’ names in a list on one line, e.g.,\ngroup: trevor ruiz, yan lashchev\nCommit and push changes\nThen on the main screen, without pulling new changes, create a commit with the names shown differently somehow, such as last, first, or initials, or spanning multiple lines with one name per line.\nAttempt to push the commit. Your client will detect ‘upstream’ changes on the remote repository and prompt you to pull changes.\nAttempt to pull the changes. The client will then report a merge conflict and prompt you to resolve the conflict and commit changes. GitHub Desktop in particular will prompt you to open RStudio to resolve the conflict. Go ahead and follow the prompt.\n\nResolve a merge conflict\nYou will see a version of the file with the conflict that shows <<<<HEAD … >>>> followed by a long alphanumeric string. Within the angle brackets the two conflicting versions of the file will be shown, separated by ===== .\n\nAgree with your table on one version of the README file (or another representation of your names).\nCommit and push the change.\n\n\n\nWhen detected, merge conflicts must be resolved with a commit that takes precedence over the conflicting commits. You can read more about resolving merge conflicts here."
  },
  {
    "objectID": "materials/activities/github-basics.html#checklist",
    "href": "materials/activities/github-basics.html#checklist",
    "title": "GitHub basics",
    "section": "Checklist",
    "text": "Checklist\n\nOn github.com, your group-sandbox repository has a directory called class-activity containing a copy of this activity and one markdown file for each group member with two fun facts about them.\nThe repository has only one open branch.\nThe README file lists each group member’s name.\nEach group member has an up-to-date local copy of the repository."
  },
  {
    "objectID": "materials/course-materials.html",
    "href": "materials/course-materials.html",
    "title": "Course materials",
    "section": "",
    "text": "Objectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\n\n\nThursday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng, R. D., & Parker, H. S. (2022). Perspective on data science. Annual Review of Statistics and Its Application, 9, 1-20. (access online via UCSB library);\nprepare a reading response\n\n\n\n\n\n\nTuesday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity]\nThursday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\n\n\nTuesday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nThursday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Friday, October 14, 11:59 PM PST [accept via GH classroom here]\n\n\n\n\n\n\n\nTuesday meeting: introducing biomarker data; multiple testing [slides]\nSection meeting: iteration strategies [activity]"
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html",
    "href": "materials/labs/lab1-setup/lab1-setup.html",
    "title": "Course technology overview",
    "section": "",
    "text": "Read this and complete all instructions in the ‘action’ boxes during your lab section. Your TA will walk you through the activity and help to troubleshoot issues and answer any questions along the way.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "href": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "title": "Course technology overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTo complete the activity you’ll need to:\n\nhave all of the software listed on the course technology page installed;\nfind (or create) your GitHub account credentials (if you are creating an account for the first time, see advice on choosing a username).\n\n\n\n\n\n\n\nAction\n\n\n\nPreparations:\n\nLog in to your GitHub account.\nOpen your GitHub client.\nOpen a new session in RStudio.\nCreate a class folder for PSTAT197 somewhere on your machine, e.g., ~/documents/pstat197."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "href": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "title": "Course technology overview",
    "section": "RStudio projects",
    "text": "RStudio projects\nFirst we’ll get acquainted with the basic functionality of the RStudio IDE and the use of projects as a means of organizing files. If you’ve already used RStudio, great – this will still serve to introduce you to how we’ll use RStudio projects in this class.\n\nRStudio Setup\n\nYour TA will briefly review the (default) layout of the RStudio IDE. You should be able to identify/find the following:\n\nconsole\nterminal\nfile navigator\nenvironment\nhistory\n\nWe’ll use several R packages throughout the quarter. Some of these we will install on the go, but we can install several that we’ll rely on now.\n\n\n\n\n\n\nAction\n\n\n\nInstall packages\nNavigate to the console and copy-paste the following commands. You only need to do this once. This will take a minute or two to complete.\n\n\n\n# package install list \nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R'\nsource(url)\n\n# clear environment\nrm(list = ls())\n\n\n\nCreate a local project\nProjects are a means of keeping your work organized. When you create a project in a directory on your local machine, RStudio keeps track of project metadata, history, and the working environment so that every time you open the project you see whatever you had open when you last closed it.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new project:\n\nSelect File > New project\nCreate the project in a new directory as a subdirectory of your class folder\nName it example-project\n\nComment: when naming files it’s good practice to avoid spaces, special characters, and the like. A naming convention we try to follow: choose a descriptive name comprising 1-3 words or common abbreviations separated by hyphens.\n\n\nTake a moment to observe the file navigator. It should consist of a single example-project.Rproj file.\n\n\nAdd content\nWe may as well populate the project with a few files – so let’s add a dataset and write a short script, as if we’re just starting a data analysis.\n\n\n\n\n\n\nAction\n\n\n\nRetrieve data and store a local copy\n\nOpen a new script: File > New File > R Script\nIn the navigator, create a folder called data and a folder called scripts\nCopy and paste the code chunk below into your script.\nExecute once, then save in the scripts folder as data-retrieval.R and close\n\n\n\n\nlibrary(tidyverse)\n\n# retrieve pollution data\nurl <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab1-setup/data/pollution.csv'\npollution <- read_csv(url)\n\n# write as csv to file\nwrite_csv(pollution, file = 'data/pollution.csv')\n\n# clear environment\nrm(list = ls())\n\n\nNext, we’ll do a simple regression analysis.\n\n\n\n\n\n\nAction\n\n\n\nCreate a script\n\nCreate a new script as before\nCopy-paste the code chunk below into your script\nExecute once and examine the results\nSave in the scripts folder as slr-analysis.R\n\n\n\n\nlibrary(tidyverse)\n\n# load data\npollution <- read_csv('data/pollution.csv')\n\n# examine scatterplot with SLR fit\nggplot(pollution,\n       aes(x = log(SO2), y = Mort)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n# compute SLR fit\nfit <- lm(Mort ~ log(SO2), data = pollution)\nbroom::tidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    887.      17.6      50.4  1.37e-49\n2 log(SO2)        16.7      4.99      3.35 1.40e- 3\n\n# interpret\nfit_ci <- confint(fit, parm = 'log(SO2)')*log(1.2)\n\npaste('With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between', \n      round(fit_ci[1], 2), \n      'and', \n      round(fit_ci[2], 2), \n      'per 100k', sep = ' ') %>% \n  print()\n\n[1] \"With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between 1.23 and 4.87 per 100k\"\n\n\n\nCongrats on your first project! You can close the RStudio session now.\nWe’ll be using projects structured much like what you just set up, but with one catch: we’ll link up our RStudio projects with shared repositories so that we can all collaborate on the same set of project files."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "href": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "title": "Course technology overview",
    "section": "GitHub repositories",
    "text": "GitHub repositories\n\nWe will be distributing course assignments as repositories via GitHub Classroom. A repository is simply a storage space.\nHere we’ll walk you through how to access and copy the files in a repository just as you will for course assignments. The first step is to accept an assignment through a link we’ve given to you – this will create a repository for you with the files we intend for you to have.\nFor now, we’ll make a ‘group sandbox’ that you can play in during our next class meeting.\n\n\n\n\n\n\nAction\n\n\n\nAccept an assignment in GitHub Classroom\n\nFollow the link to accept the group-sandbox assignment. Since it’s a group assignment, you will be prompted to join a team.\nJoin the team for the table you sat at during last class meeting. If you don’t remember, check your attendance form receipt.\n\nYou should be directed to a team repository on github.com. You may need to refresh your browser. Keep this window open; you will need the URL.\n\n\n\nGit and GitHub\nAt some point in time – possibly quite recently – you had to install Git on your local machine, as well as create a GitHub account. So, Git and GitHub are two different things.\nGit is version control software that enables you to systematically track and control file changes within a repository – a collection of files possibly with some directory structure. (The definition of ‘repository’ is simply ‘storage place’.)\nGitHub is an online platform for hosting repositories remotely. Anyone with access to a repository can make changes to files in the repository, and this enables multiple people to collaborate on code.\n\n\nlocal <> remote\nUsually remote repositories are not updated directly because contributors need to execute codes to test their changes and the remote server that hosts the repository is not equipped to do this.\nInstead, contributors will prepare changes on their own machine where they can test them, and then update the remote repository once their changes are complete.\nThis process of implementing file changes in a repository involves communicating information between local and remote locations. For this purpose a local copy of the remote repository is needed.\n\n\nCloning a repository\nIn Git lingo, a clone is a local copy of a remote repository. Creating a clone copies files and establishes the link between local and remote repositories so that changes can be sent to and received from the remote repository. You only need to create a clone once.\nTo clone a repository, all one needs is:\n\nthe remote location URL;\nthe local destination where the clone will be created;\npermission from the repository owner, if private.\n\nHere you’ll clone the group sandbox repository you just created/joined. You will need the URL; if you happened to close the page when you accepted the assignment earlier, you should be able to find the repository from your home page on github.com.\n\n\n\n\n\n\n\nAction\n\n\n\nClone the sandbox repository:\n\nOpen your GitHub client (GitKracken or GitHub Desktop or similar) and ensure you are logged in to your GitHub account.\nLook for a ‘Clone Repo’ menu item or similar and simply input the URL and the place you’d like to clone it; proceed through any prompts.\nCheck your file navigator to confirm that the repository files were copied.\n\n\n\n\nAn alternative possibility is to create the clone using a terminal command. In the terminal, navigate to the desired destination, and input:\ngit clone https://github.com/USERNAME/REPONAME\n\n\n\n\n\n\nRemarks\n\n\n\nOn terminal commands:\n\nIt’s recommended to manage Git actions through a visual client, as it’s much easier to see and understand what’s happening.\nHowever, if you know exactly what you’re doing, executing simple actions via Git bash in the terminal can be more efficient at times.\nFor example, you can keep a terminal open in RStudio and manage your repository workflow from there, without having to toggle between environments.\nTry experimenting with terminal commands from RStudio after you have a little experience with basic Git actions."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "href": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "title": "Course technology overview",
    "section": "Checklist",
    "text": "Checklist\nHave you completed all of the activity action items?\n\nInstall software: R, RStudio, Git, and a GitHub client\nCreate a GitHub account\nInstall R packages that will be used frequently\nCreate a local project in RStudio\nAccept the group sandbox assignment on GitHub Classroom\nClone the group sandbox repo"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "title": "Tidyverse basics",
    "section": "",
    "text": "Read through the R Basics section and then complete all actions in the Tidyverse basics section. This lab is for your own benefit and no submission is expected.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "title": "Tidyverse basics",
    "section": "Data types",
    "text": "Data types\nThere are five main data types in R.\nNumeric (double- or single-precision floating point) data represent real numbers. Numeric data are abbreviated num and by default are stored as double-precision floating point.\n\n# a number\n4.5\n\n[1] 4.5\n\n# check structure\nstr(4.5)\n\n num 4.5\n\n# stored as double\nis.double(4.5)\n\n[1] TRUE\n\n\nInteger data are integers. For the most part they behave like numeric data, except occupy less memory, which can in some cases be convenient. To distinguish integers from doubles, R uses a trailing L after values; the data type is abbreviated int.\n\n# an integer\n4L\n\n[1] 4\n\n# check structure\nstr(4L)\n\n int 4\n\n\nLogical data are binary and represented in R as having values TRUE and FALSE. They are abbreviated logi in R. Often they are automatically coerced to integer data with values 0 (false) and 1 (true) to perform arithmetic and other operations.\n\n# logical value\nTRUE\n\n[1] TRUE\n\n# check structure\nstr(TRUE)\n\n logi TRUE\n\n# arithmetic\nTRUE + FALSE\n\n[1] 1\n\n# check structure\nstr(FALSE + FALSE)\n\n int 0\n\n\nCharacter data represent strings of text and are sometimes called ‘strings’. They are abbreviated chr in R and values are surrounded by quotation marks; this distinguishes, for example, the character 4 from the number 4. Single quotations can be used to input strings as well as double quotations. Arithmetic is not possible with strings for obvious reasons.\n\n# a character string\n'yay'\n\n[1] \"yay\"\n\n# check structure\nstr('yay')\n\n chr \"yay\"\n\n# string arithmetic won't work\n'4' + '1'\n\nError in \"4\" + \"1\": non-numeric argument to binary operator\n\n# but can be performed after coercing character to string\nas.numeric('4') + as.numeric('1')\n\n[1] 5\n\n\nFactor data represent categorical variables. In R these are encoded numerically according to the number of ‘levels’ of the factor, which represent the unique values of the categorical variable, and each level is labeled. R will print the labels, not the levels, of factors; the data type is abbreviated fct.\n\n# a factor\nfactor(1, levels = c(1, 2), labels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# less verbose definition\nfactor('blue', levels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# check structure\nstr(factor('blue', levels = c('blue', 'red')))\n\n Factor w/ 2 levels \"blue\",\"red\": 1\n\n\nUsually factors won’t be defined explicitly, but instead interpreted from character data. The levels and labels of factors can be manipulated using a variety of helper functions."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "title": "Tidyverse basics",
    "section": "Object classes",
    "text": "Object classes\nThe most basic type of object in R is a vector. Vectors are concatenations of data values of the same type. They are defined using the concatenation operator c() and are indexed by consecutive integers; subvectors can be retrieved by specifying the indices between square brackets.\n\n# numeric vector\nc(1, 4, 7)\n\n[1] 1 4 7\n\n# character vector\nc('blue', 'red')\n\n[1] \"blue\" \"red\" \n\n# indexing\nc(1, 4, 7)[1]\n\n[1] 1\n\nc(1, 4, 7)[2]\n\n[1] 4\n\nc(1, 4, 7)[3]\n\n[1] 7\n\nc(1, 4, 7)[2:3]\n\n[1] 4 7\n\nc(1, 4, 7)[c(1, 3)]\n\n[1] 1 7\n\n\nUsually objects are assigned names for easy retrieval. Vectors will not show any special object class if the structure is examined; str() will simply return the data type, index range, and the values.\n\n# assign a name\nmy_vec <- c(1, 4, 7)\n\n# check structure\nstr(my_vec)\n\n num [1:3] 1 4 7\n\n\nNext up in complexity are arrays. These are blocks of data values of the same type indexed along two or more dimensions. For arrays, str() will return the data type, index structure, and data values; when printed directly, data values are arranged according to the indexing.\n\n# an array\nmy_ary <- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 4))\n\nmy_ary\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nstr(my_ary)\n\n num [1:2, 1:4] 1 2 3 4 5 6 7 8\n\n# another array\nmy_oth_ary <- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 2, 2))\n\nmy_oth_ary\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nstr(my_oth_ary)\n\n num [1:2, 1:2, 1:2] 1 2 3 4 5 6 7 8\n\n\nFor arrays, elements can be retrieved by index coordinates, and slices can be retrieved by leaving index positions blank, which will return all elements along the corresponding indices.\n\n# one element\nmy_ary[1, 2]\n\n[1] 3\n\n# one element\nmy_oth_ary[1, 2, 1]\n\n[1] 3\n\n# a slice (second row)\nmy_ary[2, ]\n\n[1] 2 4 6 8\n\n# a slice (first layer)\nmy_oth_ary[ , , 1]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNext there are lists, which are perhaps the most flexible data structure. A list is an indexed collection of any objects.\n\n# a list\nlist('cat', c(1, 4, 7), TRUE)\n\n[[1]]\n[1] \"cat\"\n\n[[2]]\n[1] 1 4 7\n\n[[3]]\n[1] TRUE\n\n# a named list\nlist(animal = 'cat',\n     numbers = c(1, 4, 7),\n     short = TRUE)\n\n$animal\n[1] \"cat\"\n\n$numbers\n[1] 1 4 7\n\n$short\n[1] TRUE\n\n\nList elements can be retrieved by index in double square brackets, or by name.\n\n# assign a name\nmy_lst <- list(animal = 'cat',\n               numbers = c(1, 4, 7),\n               short = TRUE)\n\n# check structure\nstr(my_lst)\n\nList of 3\n $ animal : chr \"cat\"\n $ numbers: num [1:3] 1 4 7\n $ short  : logi TRUE\n\n# retrieve an element\nmy_lst[[1]]\n\n[1] \"cat\"\n\n# equivalent\nmy_lst$animal\n\n[1] \"cat\"\n\n\nFinally, data frames are type-heterogeneous lists of vectors of equal length. More informally, they are 2D arrays with columns of differing data types. str() will essentially show the list structure; but when printed, data frames will appear arranged in a table.\n\n# a data frame\nmy_df <- data.frame(animal = c('cat', 'hare', 'tortoise'),\n                    has.fur = c(TRUE, TRUE, FALSE),\n                    weight.lbs = c(9.1, 8.2, 22.7))\n\nstr(my_df)\n\n'data.frame':   3 obs. of  3 variables:\n $ animal    : chr  \"cat\" \"hare\" \"tortoise\"\n $ has.fur   : logi  TRUE TRUE FALSE\n $ weight.lbs: num  9.1 8.2 22.7\n\nmy_df\n\n    animal has.fur weight.lbs\n1      cat    TRUE        9.1\n2     hare    TRUE        8.2\n3 tortoise   FALSE       22.7\n\n\nThe data frame is the standard object type for representing datasets in R. For the most part, modern computing in R is designed around the data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "title": "Tidyverse basics",
    "section": "Packages",
    "text": "Packages\nR packages are add-ons that can include special functions, datasets, object classes, and the like. They are published software and can be installed using install.packages('PACKAGE NAME') and, once installed, loaded via library('PACKAGE NAME') or require('PACKAGE NAME')."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "title": "Tidyverse basics",
    "section": "Concepts",
    "text": "Concepts\nThe tidyverse is a collection of packages for data manipulation, visualization, and statistical modeling. Some are specialized, such as forcats or lubridate, which contain functions for manipulating factors and dates and times, respectively. The packages share some common underyling principles.\n\nPackages are built around the data frame\nFunctions are designed to work with the pipe operator %>%\nPackages facilitate readable code\n\nThe tidyverse facilitates programming in readable sequences of steps that are performed on dataframe. For example:\n\nmy_df %>% STEP1() %>% STEP2() %>% STEP3()\n\nIf it helps, imagine that step 1 is defining a new variable, step 2 is selecting a subset of columns, and step 3 is fitting a model of some kind."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "title": "Tidyverse basics",
    "section": "Tibbles",
    "text": "Tibbles\ntidyverse packages leverage a slight generalization of the data frame called a tibble. For the most part, tibbles behave as data frames do, but they are slightly more flexible in ways you’ll encounter later.\nFor now, think of a tibble as just another name for a data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "title": "Tidyverse basics",
    "section": "The pipe operator %>%",
    "text": "The pipe operator %>%\nIn short, x %>% f(y) is equivalent to f(x, y) .\nIn other words, the pipe operator ‘pipes’ the result of the left-hand operation into the first argument of the right-hand function.\n\n# a familiar example\nmy_vec <- c(1, 2, 5) \nstr(my_vec)\n\n num [1:3] 1 2 5\n\n# use the pipe operator instead\nmy_vec %>% str()\n\n num [1:3] 1 2 5"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "title": "Tidyverse basics",
    "section": "dplyr verbs",
    "text": "dplyr verbs\nThe dplyr package contains functions for manipulating data frames (tibbles). The functions are named with verbs that describe common operations.\n\nCore verbs\n\n\n\n\n\n\nAction\n\n\n\nFor each verb listed below, copy the code chunk into your script and execute.\nGo through the list with your neighbor and check your understanding by describing what the code example accomplishes.\n\n\nfilter – filter the rows of a data frame according to a condition and return a subset of rows meeting that condition\n\n# filter rows\nbackground %>%\n  filter(math.comf > 3)\n\n# A tibble: 37 × 30\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         <dbl> <chr>         <dbl> <chr>         <dbl> <chr>         <dbl>\n 1           5 Int               3 Adv               4 Int               3\n 2           7 Int               3 Adv               4 Int               3\n 3           8 Adv               4 Adv               5 Adv               4\n 4          12 Int               5 Int               4 Int               4\n 5          13 Int               4 Adv               4 Adv               4\n 6          15 Adv               3 Adv               4 Adv               4\n 7          17 Int               3 Int               4 Int               4\n 8          18 Int               4 Adv               5 Adv               5\n 9          19 Adv               5 Adv               5 Adv               5\n10          20 Adv               4 Int               4 Adv               5\n# … with 27 more rows, and 23 more variables: updv.num <chr>, dom <chr>,\n#   consent <chr>, PSTAT100 <chr>, `PSTAT120A-B` <chr>, PSTAT126 <chr>,\n#   PSTAT127 <chr>, PSTAT134 <chr>, CS9 <chr>, PSTAT131 <chr>, PSTAT174 <chr>,\n#   LING104 <chr>, LING105 <chr>, PSTAT115 <chr>, PSTAT122 <chr>,\n#   `PSTAT160A-B` <chr>, CS16 <chr>, `CS5A-B` <chr>, POLS15 <chr>,\n#   LING110 <chr>, `CS130A-B` <chr>, `CS165A-B` <chr>, rsrch <lgl>\n\n\nselect – select a subset of columns from a data frame\n\n# select a column\nbackground %>%\n  select(math.comf)\n\n# A tibble: 59 × 1\n   math.comf\n       <dbl>\n 1         3\n 2         3\n 3         3\n 4         3\n 5         4\n 6         4\n 7         5\n 8         3\n 9         3\n10         4\n# … with 49 more rows\n\n\npull – extract a single column from a data frame\n\n# pull a column\nbackground %>%\n  pull(rsrch)\n\n [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n[25] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[37] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[49]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nmutate – define a new column as a function of existing columns\n\n# define a new variable\nbackground %>%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3)\n\n# A tibble: 59 × 31\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         <dbl> <chr>         <dbl> <chr>         <dbl> <chr>         <dbl>\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 24 more variables: updv.num <chr>, dom <chr>,\n#   consent <chr>, PSTAT100 <chr>, `PSTAT120A-B` <chr>, PSTAT126 <chr>,\n#   PSTAT127 <chr>, PSTAT134 <chr>, CS9 <chr>, PSTAT131 <chr>, PSTAT174 <chr>,\n#   LING104 <chr>, LING105 <chr>, PSTAT115 <chr>, PSTAT122 <chr>,\n#   `PSTAT160A-B` <chr>, CS16 <chr>, `CS5A-B` <chr>, POLS15 <chr>,\n#   LING110 <chr>, `CS130A-B` <chr>, `CS165A-B` <chr>, rsrch <lgl>,\n#   avg.comf <dbl>\n\n\nThese operations can be chained together, for example:\n\n# sequence of verbs\nbackground %>%\n  filter(stat.prof == 'Adv') %>%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %>%\n  select(avg.comf, rsrch) \n\n# A tibble: 36 × 2\n   avg.comf rsrch\n      <dbl> <lgl>\n 1     4    FALSE\n 2     4.33 TRUE \n 3     3.67 FALSE\n 4     4    FALSE\n 5     3.67 FALSE\n 6     4.67 TRUE \n 7     5    TRUE \n 8     4.33 TRUE \n 9     4.33 TRUE \n10     4    TRUE \n# … with 26 more rows\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nWrite a chain of verbs in order to find the proficiency ratings of all respondents with research experience and 6-8 upper division courses.\nWrite a chain of verbs in order to find the proficiency ratings of all respondents without research experience and the same number of upper division courses\nCompare results and discuss with your neighbor: do these suggest any patterns?\n\n\n\n\n\nSummaries\nSummaries are easily computed across rows using summarize() . So if for example we want to use the filtering and selection from before to find the proportion of advanced students in statistics with research experience, use:\n\n# a summary\nbackground %>%\n  filter(stat.prof == 'Adv') %>%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %>%\n  select(avg.comf, rsrch) %>%\n  summarize(prop.rsrch = mean(rsrch))\n\n# A tibble: 1 × 1\n  prop.rsrch\n       <dbl>\n1      0.611\n\n# equivalent\nbackground %>%\n  filter(stat.prof == 'Adv') %>%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %>%\n  select(avg.comf, rsrch) %>%\n  pull(rsrch) %>%\n  mean()\n\n[1] 0.6111111\n\n\nThe advantage of summarize , however, is that multiple summaries can be computed at once:\n\nbackground %>%\n  filter(stat.prof == 'Adv') %>%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %>%\n  select(avg.comf, rsrch) %>%\n  summarize(prop.rsrch = mean(rsrch),\n            med.comf = median(avg.comf))\n\n# A tibble: 1 × 2\n  prop.rsrch med.comf\n       <dbl>    <dbl>\n1      0.611        4\n\n\nThe variant summarize_all computes the same summary across all columns. (Notice the use of the helper verb contains() to select all columns containing a particular string.)\n\n# average comfort levels across all students\nbackground %>%\n  select(contains('comf')) %>%\n  summarise_all(.funs = mean)\n\n# A tibble: 1 × 3\n  prog.comf math.comf stat.comf\n      <dbl>     <dbl>     <dbl>\n1      3.97      3.85      4.08\n\n\nGrouped summaries are summaries computed separately among subsets of observations. To define a grouping structure using an existing column, use group_by() . Notice the ‘groups’ attribute printed with the output.\n\n# create a grouping\nbackground %>%\n  group_by(stat.prof)\n\n# A tibble: 59 × 30\n# Groups:   stat.prof [3]\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         <dbl> <chr>         <dbl> <chr>         <dbl> <chr>         <dbl>\n 1           1 Adv               5 Int               3 Adv               4\n 2           2 Int               5 Int               3 Int               4\n 3           3 Int               3 Int               3 Int               4\n 4           4 Int               3 Int               3 Int               3\n 5           5 Int               3 Adv               4 Int               3\n 6           7 Int               3 Adv               4 Int               3\n 7           8 Adv               4 Adv               5 Adv               4\n 8           9 Int               3 Int               3 Int               3\n 9          10 Int               4 Int               3 Adv               4\n10          12 Int               5 Int               4 Int               4\n# … with 49 more rows, and 23 more variables: updv.num <chr>, dom <chr>,\n#   consent <chr>, PSTAT100 <chr>, `PSTAT120A-B` <chr>, PSTAT126 <chr>,\n#   PSTAT127 <chr>, PSTAT134 <chr>, CS9 <chr>, PSTAT131 <chr>, PSTAT174 <chr>,\n#   LING104 <chr>, LING105 <chr>, PSTAT115 <chr>, PSTAT122 <chr>,\n#   `PSTAT160A-B` <chr>, CS16 <chr>, `CS5A-B` <chr>, POLS15 <chr>,\n#   LING110 <chr>, `CS130A-B` <chr>, `CS165A-B` <chr>, rsrch <lgl>\n\n\nSometimes it can be helpful to simply count the observations in each group:\n\n# count observations\nbackground %>%\n  group_by(stat.prof) %>%\n  count()\n\n# A tibble: 3 × 2\n# Groups:   stat.prof [3]\n  stat.prof     n\n  <chr>     <int>\n1 Adv          36\n2 Beg           2\n3 Int          21\n\n\nTo compute a grouped summary, first group the data frame and then specify the summary of interest:\n\n# a grouped summary\nbackground %>%\n  group_by(stat.prof) %>%\n  select(contains('.comf')) %>%\n  summarize_all(.funs = mean)\n\n# A tibble: 3 × 4\n  stat.prof prog.comf math.comf stat.comf\n  <chr>         <dbl>     <dbl>     <dbl>\n1 Adv            4.06      4.14      4.39\n2 Beg            3.5       2.5       2.5 \n3 Int            3.86      3.48      3.71\n\n\n\n\n\n\n\n\nAction\n\n\n\nGrouped summaries\n\nCompute the median comfort level of all students in each subject area.\nCompute the median comfort level of all students in each subject area after grouping by number of upper division classes taken.\nCompare and discuss with your neighbor: do you notice any interesting patterns?"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "title": "Tidyverse basics",
    "section": "tidyr verbs",
    "text": "tidyr verbs\nIn general, tidyr verbs reshape data frames in various ways. For now, we’ll just cover two tidyr verbs.\nSuppose we want to calculate multiple summaries of multiple variables using the techniques above. By default, the output is one row with one column for each summary/variable combination:\n\n# many variables, many summaries\ncomf_sum <- background %>%\n  select(contains('comf')) %>%\n  summarise_all(.funs = list(mean = mean, \n                             median = median,\n                             min = min, \n                             max = max))\n\ncomf_sum\n\n# A tibble: 1 × 12\n  prog.comf_mean math.comf_mean stat.comf_mean prog.comf_median math.comf_median\n           <dbl>          <dbl>          <dbl>            <dbl>            <dbl>\n1           3.97           3.85           4.08                4                4\n# … with 7 more variables: stat.comf_median <dbl>, prog.comf_min <dbl>,\n#   math.comf_min <dbl>, stat.comf_min <dbl>, prog.comf_max <dbl>,\n#   math.comf_max <dbl>, stat.comf_max <dbl>\n\n\nIt would be much better to reshape this into a table. gather will reshape the data frame from wide format to long format by ‘gathering’ the columns together.\n\n# gather columns into long format\ncomf_sum %>% gather(stat, val) \n\n# A tibble: 12 × 2\n   stat               val\n   <chr>            <dbl>\n 1 prog.comf_mean    3.97\n 2 math.comf_mean    3.85\n 3 stat.comf_mean    4.08\n 4 prog.comf_median  4   \n 5 math.comf_median  4   \n 6 stat.comf_median  4   \n 7 prog.comf_min     3   \n 8 math.comf_min     2   \n 9 stat.comf_min     2   \n10 prog.comf_max     5   \n11 math.comf_max     5   \n12 stat.comf_max     5   \n\n\nThis is a little better, but it would be more legible in a 2x2 table. We can separate the ‘stat’ variable that has the column names into two columns:\n\n# separate into rows and columns\ncomf_sum %>%\n  gather(stat, val) %>%\n  separate(stat, into = c('variable', 'stat'), sep = '_') \n\n# A tibble: 12 × 3\n   variable  stat     val\n   <chr>     <chr>  <dbl>\n 1 prog.comf mean    3.97\n 2 math.comf mean    3.85\n 3 stat.comf mean    4.08\n 4 prog.comf median  4   \n 5 math.comf median  4   \n 6 stat.comf median  4   \n 7 prog.comf min     3   \n 8 math.comf min     2   \n 9 stat.comf min     2   \n10 prog.comf max     5   \n11 math.comf max     5   \n12 stat.comf max     5   \n\n\nAnd then spread the stat column over a few rows, resulting in a table where the rows are the variables and the columns are the summaries:\n\n# spread into table\ncomf_sum %>%\n  gather(stat, val) %>%\n  separate(stat, into = c('variable', 'stat'), sep = '_') %>%\n  spread(stat, val)\n\n# A tibble: 3 × 5\n  variable    max  mean median   min\n  <chr>     <dbl> <dbl>  <dbl> <dbl>\n1 math.comf     5  3.85      4     2\n2 prog.comf     5  3.97      4     3\n3 stat.comf     5  4.08      4     2"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "title": "Tidyverse basics",
    "section": "ggplot",
    "text": "ggplot\nThe ggplot package is for data visualization. The syntax takes some getting used to if you haven’t seen it before. We’ll just look at one example.\nSuppose we want to summarize the prior coursework in the class.\n\n# summary of classes taken\nclasses <- background %>%\n  select(11:29) %>%\n  mutate_all(~factor(.x, levels = c('no', 'yes'))) %>%\n  mutate_all(~as.numeric(.x) - 1) %>%\n  summarize_all(mean) %>%\n  gather(class, proportion)\n\nclasses\n\n# A tibble: 19 × 2\n   class       proportion\n   <chr>            <dbl>\n 1 PSTAT100        0.271 \n 2 PSTAT120A-B     0.983 \n 3 PSTAT126        0.932 \n 4 PSTAT127        0.169 \n 5 PSTAT134        0.305 \n 6 CS9             0.576 \n 7 PSTAT131        0.424 \n 8 PSTAT174        0.169 \n 9 LING104         0.0339\n10 LING105         0.0339\n11 PSTAT115        0.102 \n12 PSTAT122        0.356 \n13 PSTAT160A-B     0.458 \n14 CS16            0.492 \n15 CS5A-B          0.0508\n16 POLS15          0.0169\n17 LING110         0.0169\n18 CS130A-B        0.0508\n19 CS165A-B        0.0339\n\n\nWe could report the results in a table, in which case perhaps arranging in descending order may be helpful:\n\nclasses %>% arrange(desc(proportion))\n\n# A tibble: 19 × 2\n   class       proportion\n   <chr>            <dbl>\n 1 PSTAT120A-B     0.983 \n 2 PSTAT126        0.932 \n 3 CS9             0.576 \n 4 CS16            0.492 \n 5 PSTAT160A-B     0.458 \n 6 PSTAT131        0.424 \n 7 PSTAT122        0.356 \n 8 PSTAT134        0.305 \n 9 PSTAT100        0.271 \n10 PSTAT127        0.169 \n11 PSTAT174        0.169 \n12 PSTAT115        0.102 \n13 CS5A-B          0.0508\n14 CS130A-B        0.0508\n15 LING104         0.0339\n16 LING105         0.0339\n17 CS165A-B        0.0339\n18 POLS15          0.0169\n19 LING110         0.0169\n\n\nLet’s say we’d rather plot this data. We’ll put the course number on one axis and the proportion of students who took it on the other.\n\n# plot it\nclasses %>%\n  ggplot(aes(x = proportion, y = class)) +\n  geom_point()\n\n\n\n\nThese commands work by defining plot layers. In the chunk above, the first argument to ggplot() is the data. Then, aes() defines an ‘aesthetic mapping’ of the columns of the input data frame to graphical elements. This defines a set of axes. Then, a layer of points is added to the plot with geom_point() ; no arguments are needed because the geometric object (‘geom’) inherits attributes (x and y coordinates) from the aesthetic mapping.\nAgain we might prefer to arrange the classes by descending order in proportion.\n\nfig <- classes %>%\n  ggplot(aes(x = proportion, y = reorder(class, proportion))) +\n  geom_point()\n\nfig\n\n\n\n\nAnd perhaps fix the plot labels:\n\n# adjust labels\nfig + labs(x = 'proportion of class', y = '')\n\n\n\n\nNotice that ggplot allows for a plot to be stored by name and then further modified with additional layers."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "title": "Tidyverse basics",
    "section": "Checklist",
    "text": "Checklist\n\nAll actions were completed.\nAll code chunks were copied into your script.\nYour script is saved in a lab subfolder of your class directory with an associated project."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iterated computations: repeatedly performing the same computations. Here we’ll look at a few strategies for iteration in R:\nWe’ll illustrate these strategies using the biomarker data."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "title": "Iteration strategies",
    "section": "Loops",
    "text": "Loops\n\nSimple examples\nA loop is a set of instructions to be repeated a specified number of times while incrementing a flag or index value. For example:\n\nfor(i in 1:4){\n  print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nHere the instructions are:\n\ninitialize index/flag i at i = 1\nexecute code within the braces {...}\nincrement i <- i + 1\nstop after i = 4\n\nWe could make the loop a bit more verbose:\n\nflag_vals <- c(1, 2, 3, 4)\nfor(i in flag_vals){\n  out <- 2*i\n  print(out)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nNow to retain the results in memory, a storage data structure must be defined and the output of each iteration assigned to some element(s) of the storage object.\n\nrslt <- rep(NA, 4)\nfor(i in 1:4){\n  rslt[i] <- 2*i\n}\nrslt\n\n[1] 2 4 6 8\n\n\nIf we want to perform the same calculation for all values in a vector, we might do something like this:\n\nrslt <- rep(NA, 4)\ninput_vals <- c(15, 27, 3, 12.6)\nfor(i in 1:4){\n  rslt[i] <- 2*input_vals[i]\n}\nrslt\n\n[1] 30.0 54.0  6.0 25.2\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\nWhy does the following loop produce an NA ?\n\nrslt <- rep(NA, 4)\ninput_vals <- rnorm(n = 3)\nfor(i in 1:4){\n  rslt[i] <- 2*input_vals[i]\n}\n\nrslt\n\n[1] -3.3664950 -0.5316422  1.2368258         NA\n\n\n\n\nLoops are substantially similar in any programming language but usually not optimized for performance. Additionally, they are somewhat verbose and hard to read due to explicit use of indexing in the syntax.\n\n\nMultiple testing with loops\nIn base R, the \\(t\\)-test is performed using t.test(...) , which takes as arguments two vectors of observations (one for each group). For instance:\n\nx <- asd %>% filter(group == 'ASD') %>% pull(CHIP)\ny <- asd %>% filter(group == 'TD') %>% pull(CHIP)\nt.test(x, y, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\n  mean of x   mean of y \n-0.04922954 -0.02672849 \n\n\nThe output is a list:\n\nt.test(x, y) %>% str()\n\nList of 10\n $ statistic  : Named num -0.188\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 152\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.851\n $ conf.int   : num [1:2] -0.259 0.214\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] -0.0492 -0.0267\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 0.12\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"x and y\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nSo if we want the p-value:\n\nt.test(x, y, var.equal = F)$p.value\n\n[1] 0.8510352\n\n\nTo calculate \\(p\\)-values for all tests using a loop, we wrap the code we used to perform one \\(t\\)-test in a for loop and add appropriate indexing. For speed, we’ll just compute the first 100 tests:\n\nn_tests <- 100\np_vals <- rep(NA, n_tests)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  p_vals[i] <- t.test(x, y, var.equal = F)$p.value\n}\n\nTo line these up with the proteins they correspond to, it’s necessary to keep track of the indexing carefully. In this case, the indexing corresponds to the order of columns. So we could create a dataframe like so:\n\ntibble(protein = colnames(asd)[2:(n_tests + 1)],\n       p = p_vals)\n\n# A tibble: 100 × 2\n   protein        p\n   <chr>      <dbl>\n 1 CHIP     0.851  \n 2 CEBPB    0.0322 \n 3 NSE      0.350  \n 4 PIAS4    0.104  \n 5 IL-10 Ra 0.0232 \n 6 STAT3    0.00183\n 7 IRF1     0.592  \n 8 c-Jun    0.0351 \n 9 Mcl-1    0.999  \n10 OAS1     0.942  \n# … with 90 more rows\n\n\nAlternatively, we could have set up the loop to output this result:\n\nn_tests <- 100\nrslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  rslt$p[i] <- t.test(x, y, var.equal = F)$p.value\n}\n\n\n\n\n\n\n\nAction\n\n\n\nFollow the example above to write a loop that stores both the \\(p\\)-values and the estimated differences for the first 50 proteins."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "title": "Iteration strategies",
    "section": "Apply family",
    "text": "Apply family\n\nSimple examples\nIn R, the apply family of functions allows one to efficiently iterate a function over an index set. So, to execute our simple for loop using apply , we could do something like this:\n\nvals <- rnorm(n = 4)\nsimple_fn <- function(x){2*x}\nlapply(vals, simple_fn)\n\n[[1]]\n[1] 1.062206\n\n[[2]]\n[1] 1.619845\n\n[[3]]\n[1] 5.018638\n\n[[4]]\n[1] 0.800267\n\n\nThis applies simple_fn to each element of vals , and returns the result as a list. If we want a neater output format, we could use sapply , which is short for sort-apply:\n\nsapply(vals, simple_fn)\n\n[1] 1.062206 1.619845 5.018638 0.800267\n\n\nIn more complex settings it often makes sense to apply a function across an index set. This is very similar conceptually to a for loop, but can be a bit faster and easier to read.\n\n# apply a function to an index set\nsimple_fn_ix <- function(i){2*vals[i]}\nrslt_apply <- sapply(1:length(vals), simple_fn_ix)\n\n# equivalent for loop\nrslt_loop <- rep(NA, length(vals))\nfor(i in 1:length(vals)){\n  rslt_loop[i] <- 2*vals[i]\n}\n\n# compare\nrbind(rslt_loop, rslt_apply)\n\n               [,1]     [,2]     [,3]     [,4]\nrslt_loop  1.062206 1.619845 5.018638 0.800267\nrslt_apply 1.062206 1.619845 5.018638 0.800267"
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#t-tests-using-apply",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#t-tests-using-apply",
    "title": "Iteration strategies",
    "section": "\\(t\\)-tests using apply",
    "text": "\\(t\\)-tests using apply\nWe can use apply functions to compute \\(t\\)-tests for the proteins in the ASD data by coercing the data to a list of data frames that contain the grouping and level for each protein.\n\n# number of tests to perform\nn_tests <- 100\n\n# convert to a list\nasd_list <- asd %>% \n  select(1:(n_tests + 1)) %>%\n  pivot_longer(cols = -group,\n               names_to = 'protein',\n               values_to = 'level') %>%\n  group_by(protein) %>%\n  group_split()\n\n# first entry in list\nasd_list[[1]]\n\n# A tibble: 154 × 3\n   group protein                     level\n   <chr> <chr>                       <dbl>\n 1 ASD   14-3-3 protein beta/alpha -0.124 \n 2 ASD   14-3-3 protein beta/alpha  0.487 \n 3 ASD   14-3-3 protein beta/alpha -0.801 \n 4 ASD   14-3-3 protein beta/alpha  2.73  \n 5 ASD   14-3-3 protein beta/alpha  1.24  \n 6 ASD   14-3-3 protein beta/alpha  0.250 \n 7 ASD   14-3-3 protein beta/alpha  0.932 \n 8 ASD   14-3-3 protein beta/alpha  0.0873\n 9 ASD   14-3-3 protein beta/alpha  0.213 \n10 ASD   14-3-3 protein beta/alpha  0.157 \n# … with 144 more rows\n\n\nThe function t.test(...) can also perform the test using a formula of the form y ~ x and a data frame containing x and y, as below.\n\nt.test(level ~ group, data = asd_list[[1]])\n\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -1.5671, df = 150.2, p-value = 0.1192\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.54341111  0.06269287\nsample estimates:\nmean in group ASD  mean in group TD \n       -0.1341683         0.1061909 \n\n\nIf we just want the \\(p\\)-value again, we can wrap this code in a function whose argument is the index i . This function will return the \\(p\\)-value for the \\(i\\)th protein.\n\n# p value for ith protein\ntt_fn <- function(i){\n  t.test(level ~ group, data = asd_list[[i]])$p.value\n}\n\n# check\ntt_fn(1)\n\n[1] 0.1191888\n\n\nNow to perform many tests, we can simply iterate this function over an index:\n\nsapply(1:n_tests, tt_fn)\n\n  [1] 1.191888e-01 2.972829e-01 8.297144e-01 3.034583e-02 8.136635e-01\n  [6] 9.517828e-01 3.553359e-01 2.671529e-03 6.458878e-01 3.915314e-04\n [11] 1.759142e-01 3.505666e-02 5.095419e-01 4.788545e-07 2.862286e-01\n [16] 5.714296e-01 7.052780e-03 3.220583e-02 8.510352e-01 1.267133e-03\n [21] 9.482110e-03 1.293157e-04 7.804081e-03 2.208460e-04 1.407044e-01\n [26] 1.023033e-01 8.995855e-02 1.578665e-02 3.113212e-04 2.920587e-02\n [31] 4.663516e-07 2.395764e-01 5.709433e-03 8.962287e-02 2.700053e-02\n [36] 5.357313e-01 7.392658e-01 8.665332e-01 3.538260e-02 1.956257e-04\n [41] 8.658766e-01 2.111378e-04 3.286108e-01 5.374305e-01 1.108687e-01\n [46] 1.711403e-01 4.808293e-01 6.775472e-01 3.556564e-03 2.322968e-02\n [51] 3.746226e-01 7.804488e-01 3.175372e-01 4.085249e-01 4.746117e-03\n [56] 5.917788e-01 3.748021e-02 6.433125e-01 1.121721e-03 3.234610e-03\n [61] 4.154758e-03 1.669733e-03 1.726578e-03 9.990017e-01 7.100178e-04\n [66] 4.811425e-01 8.978465e-01 3.503310e-01 9.423978e-01 3.925728e-01\n [71] 3.025965e-01 4.511875e-02 4.219360e-01 2.117196e-01 1.036412e-01\n [76] 5.590746e-01 8.148983e-01 1.399029e-02 5.096269e-04 9.145121e-02\n [81] 3.331394e-02 4.350959e-01 8.721647e-01 3.266951e-03 2.704495e-01\n [86] 4.929196e-01 1.010954e-03 3.144033e-01 7.933758e-04 1.929813e-03\n [91] 6.104791e-02 1.832399e-03 1.333513e-02 6.412884e-01 2.605232e-02\n [96] 4.130732e-01 2.579393e-01 4.096623e-01 2.925137e-01 5.866341e-01\n\n\nYou might have noticed this was much faster than the loop. We can time it:\n\nstart <- Sys.time()\nrslt <- sapply(1:n_tests, tt_fn)\nend <- Sys.time()\n\nend - start\n\nTime difference of 0.209661 secs\n\n\nAnd compare with the for loop:\n\nstart <- Sys.time()\nn_tests <- 100\nrslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)\n  y <- asd %>% filter(group == 'TD') %>% pull(i + 1)\n  rslt$p[i] <- t.test(x, y, var.equal = F)$p.value\n}\nend <- Sys.time()\n\nend - start\n\nTime difference of 11.63856 secs\n\n\n\n\n\n\n\n\nAction"
  },
  {
    "objectID": "materials/slides/week0-intro.html#before-we-begin",
    "href": "materials/slides/week0-intro.html#before-we-begin",
    "title": "Course orientation",
    "section": "Before we begin…",
    "text": "Before we begin…\n\nConfer with your table and choose a word of the day. Agree on spelling.\nPlease sign in using the attendance reporting form found here:\nhttps://pstat197.github.io/pstat197a/about/links.html"
  },
  {
    "objectID": "materials/slides/week0-intro.html#welcome",
    "href": "materials/slides/week0-intro.html#welcome",
    "title": "Course orientation",
    "section": "Welcome",
    "text": "Welcome\nPSTAT197A/CMPSC190DD is the first course in UCSB’s year-long data science capstone sequence.\n\nAudience: undergraduate students of any discipline with a basic background in data science and an interest in research\nAim: prepare for an independent research or project experience"
  },
  {
    "objectID": "materials/slides/week0-intro.html#capstone-projects",
    "href": "materials/slides/week0-intro.html#capstone-projects",
    "title": "Course orientation",
    "section": "Capstone projects",
    "text": "Capstone projects\nMost students are preparing for capstone projects in winter and spring. Course foci were chosen with this in mind.\n\nProjects are varied ➜ emphasize problem patterns over methodology\nProjects are collaborative ➜ emphasize teamwork and discussion\nProjects are specialized ➜ practice independent learning based on use cases\n\nRead about past projects at https://centralcoastdatascience.org/projects"
  },
  {
    "objectID": "materials/slides/week0-intro.html#continuing-in-capstones",
    "href": "materials/slides/week0-intro.html#continuing-in-capstones",
    "title": "Course orientation",
    "section": "Continuing in capstones",
    "text": "Continuing in capstones\nContinuation in PSTAT197B-C/CMPSC190DE-DF during winter and spring:\n\nstudents admitted to this course in spring have a seat;\nstudents admitted from the waitlist are on the waitlist."
  },
  {
    "objectID": "materials/slides/week0-intro.html#outcomes",
    "href": "materials/slides/week0-intro.html#outcomes",
    "title": "Course orientation",
    "section": "Outcomes",
    "text": "Outcomes\nI hope to support all of you in:\n\nusing modern software with version control for collaboration;\nrecognizing problem patterns based on data semantics and research questions;\nidentifying and accessing resources for independent learning given a problem of interest;\ncommunicating data analysis and/or research findings."
  },
  {
    "objectID": "materials/slides/week0-intro.html#classroom-environment",
    "href": "materials/slides/week0-intro.html#classroom-environment",
    "title": "Course orientation",
    "section": "Classroom environment",
    "text": "Classroom environment\nWe are in an interactive classroom for a reason: to interact!\nLet’s acknowledge:\n\nPreparations and areas of expertise vary widely among the class\nIt’s okay not to know things\nIf you have a question, probably someone else does too"
  },
  {
    "objectID": "materials/slides/week0-intro.html#resources",
    "href": "materials/slides/week0-intro.html#resources",
    "title": "Course orientation",
    "section": "Resources",
    "text": "Resources\nAll course content is hosted on our website\nhttps://pstat197.github.io/pstat197a/"
  },
  {
    "objectID": "materials/slides/week0-intro.html#modules",
    "href": "materials/slides/week0-intro.html#modules",
    "title": "Course orientation",
    "section": "Modules",
    "text": "Modules\nThe course is configured in modules defined by a dataset and questions (much like a project).\nA module typically comprises:\n\nOne session on data introduction (lecture/discussion)\nTwo sessions on problem patterns and related methodology (lecture)\nTwo labs with related examples (section meeting)\nOne session on sharing data analysis results (discussion)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#module-content",
    "href": "materials/slides/week0-intro.html#module-content",
    "title": "Course orientation",
    "section": "Module content",
    "text": "Module content\nThe module datasets are currently as follows:\n\nClass intake survey data (exploratory/descriptive analysis)\nBiomarkers of autism (predictive modeling and variable selection)\nWeb fraud (text processing and deep learning)\nSoil temperatures (correlated data)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#group-assignments",
    "href": "materials/slides/week0-intro.html#group-assignments",
    "title": "Course orientation",
    "section": "Group assignments",
    "text": "Group assignments\nEach module you will be assigned a working group.\nYour group’s objective is to produce an analysis of the dataset:\n\nReproduce analysis presented/discussed in class meeting\nExtend the analysis by\n\napplying an alternative method that addresses the same question(s)\nor addressing a corollary question"
  },
  {
    "objectID": "materials/slides/week0-intro.html#vignettes",
    "href": "materials/slides/week0-intro.html#vignettes",
    "title": "Course orientation",
    "section": "Vignettes",
    "text": "Vignettes\nAt the end of the class in place of a fifth module you will create a vignette (short demonstration) on a topic of interest.\n\npresent a use case\nexplain methodology\ndemonstrate implementation with example code"
  },
  {
    "objectID": "materials/slides/week0-intro.html#expectations-and-assessments",
    "href": "materials/slides/week0-intro.html#expectations-and-assessments",
    "title": "Course orientation",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\n\n\nStudents are expected to:\n\nprepare for class meetings as directed;\nattend and actively participate in class and section meetings;\ncontribute meaningfully to group activities and assignments.\n\n\nStudents are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview/presentation."
  },
  {
    "objectID": "materials/slides/week0-intro.html#next-time",
    "href": "materials/slides/week0-intro.html#next-time",
    "title": "Course orientation",
    "section": "Next time",
    "text": "Next time\nWe’ll discuss:\n\ndata science as a discipline;\nthe research landscape;\nsystems and design thinking for data science."
  },
  {
    "objectID": "materials/slides/week0-intro.html#checklist",
    "href": "materials/slides/week0-intro.html#checklist",
    "title": "Course orientation",
    "section": "Checklist",
    "text": "Checklist\nComplete all of the following before our next meeting.\n\nReview all content in the about section of the course webpage.\nInstall course software and create a GitHub account.\nFill out capstone project intake form.\nRead Peng, R. D., & Parker, H. S. (2022). Perspective on data science. Annual Review of Statistics and Its Application, 9, 1-20. (access online via UCSB library).\nPrepare a reading response."
  },
  {
    "objectID": "materials/slides/week1-github.html#announcementsreminders",
    "href": "materials/slides/week1-github.html#announcementsreminders",
    "title": "Basic GitHub actions",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ntoday, sit at your table from last time\nassignment for next time:\n\nread MDSR 9.1 – 9.2\nprepare a reading response"
  },
  {
    "objectID": "materials/slides/week1-github.html#objective-for-today",
    "href": "materials/slides/week1-github.html#objective-for-today",
    "title": "Basic GitHub actions",
    "section": "Objective for today",
    "text": "Objective for today\nLearn how to interact with GitHub repositories:\n\nretrieve and submit file changes;\nexamine repository updates;\nuse branches for parallel workflow;\nresolve conflicts."
  },
  {
    "objectID": "materials/slides/week1-github.html#basic-git-actions",
    "href": "materials/slides/week1-github.html#basic-git-actions",
    "title": "Basic GitHub actions",
    "section": "Basic Git actions",
    "text": "Basic Git actions\n\nCommunication actions for moving file changes between locations"
  },
  {
    "objectID": "materials/slides/week1-github.html#branching-workflow",
    "href": "materials/slides/week1-github.html#branching-workflow",
    "title": "Basic GitHub actions",
    "section": "Branching workflow",
    "text": "Branching workflow\n\nTypical use of repository branches for development of new features"
  },
  {
    "objectID": "materials/slides/week1-github.html#activity-overview",
    "href": "materials/slides/week1-github.html#activity-overview",
    "title": "Basic GitHub actions",
    "section": "Activity overview",
    "text": "Activity overview\n\nMake individual changes to files and create ‘commits’\nCreate repository branches to enable you to work more efficiently in parallel.\nMerge branches with the main branch via pull request.\nCreate and resolve a merge conflict."
  },
  {
    "objectID": "materials/slides/week1-github.html#setup",
    "href": "materials/slides/week1-github.html#setup",
    "title": "Basic GitHub actions",
    "section": "Setup",
    "text": "Setup\n\nhave everyone open their GitHub client, the sandbox project in RStudio, and the group sandbox repository in the browser on github.com\nchoose one person to operate the table workstation; have them do the same and log in to github using their credentials\non the workstation, create a directory for the class documents/22f-pstat197a and clone the group sandbox repo into this directory"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#announcementsreminders",
    "href": "materials/slides/week1-perspectives.html#announcementsreminders",
    "title": "On projects in(volving) data science",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nJoin Slack workspace, monitor channel #22f-pstat197a for announcements.\nOffice hours by demand immediately following section and class meetings\n\nYan will hold a drop in hour in building 434 room 126 after Josh’s section\n\nInstall course software and bring your laptop to section meetings. Remember your table number from today."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "href": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "title": "On projects in(volving) data science",
    "section": "Google trends: data science",
    "text": "Google trends: data science\n\n\nData science emerged as a term of art in the last decade\nInterest exploded in the last five years"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "href": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "title": "On projects in(volving) data science",
    "section": "Origins: ‘data analysis’",
    "text": "Origins: ‘data analysis’\nTukey advocated for ‘data analysis’ as a broader field than statistics (Tukey 1962), including:\n\nstatistical theory and methodology;\nvisualization and data display techniques;\ncomputation and scalability;\nbreadth of application.\n\n\nLook famililar? Tukey’s ‘data analysis’ is proto-modern data science."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "title": "On projects in(volving) data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nIn the 1960’s and 1970’s, these concepts meant very different things.\n\nvisualization meant drawing\ncomputation meant data re-expression by hand\n\n\nBut the ideas were still somewhat radical. At the time most relied on highly reductive numerical results to interpret data:\n\nANOVA tables\nregression tables\np-values"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-boxplots",
    "href": "materials/slides/week1-perspectives.html#example-boxplots",
    "title": "On projects in(volving) data science",
    "section": "Example: boxplots",
    "text": "Example: boxplots\n\nFigure from (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "title": "On projects in(volving) data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nThe new techniques allowed for iterative investigation:\n\nformulate a question\nexamine data graphics and summaries\nadjust computations and graphics to hone in on content of interest\nrefine the question"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "href": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "title": "On projects in(volving) data science",
    "section": "Birth-to-death ratio by state",
    "text": "Birth-to-death ratio by state\nSuppose we want to explain variation in birth-to-death ratios in the U.S. 1\n\nInitial question: is population density an associated factor?\n\nThis example follows (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration",
    "href": "materials/slides/week1-perspectives.html#first-iteration",
    "title": "On projects in(volving) data science",
    "section": "First iteration",
    "text": "First iteration\n\nA first attempt"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration-1",
    "href": "materials/slides/week1-perspectives.html#first-iteration-1",
    "title": "On projects in(volving) data science",
    "section": "First iteration",
    "text": "First iteration\n\nWhat if we adjust the computation?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration",
    "href": "materials/slides/week1-perspectives.html#second-iteration",
    "title": "On projects in(volving) data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nWhat about median age instead?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration-1",
    "href": "materials/slides/week1-perspectives.html#second-iteration-1",
    "title": "On projects in(volving) data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nAdjust computations for easy linear approximation"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#third-iteration",
    "href": "materials/slides/week1-perspectives.html#third-iteration",
    "title": "On projects in(volving) data science",
    "section": "Third iteration",
    "text": "Third iteration\n\nAre there outliers?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#fourth-iteration",
    "href": "materials/slides/week1-perspectives.html#fourth-iteration",
    "title": "On projects in(volving) data science",
    "section": "Fourth iteration",
    "text": "Fourth iteration\n\nAre outliers spatially correlated?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "href": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "title": "On projects in(volving) data science",
    "section": "A bit of history",
    "text": "A bit of history\nIt’s worth noting that in the first half of the 20th century, much of statistics focused on methodology and theory for the analysis of small iid samples, and in particular:\n\ninference on means and inference on tables;\nanalysis of variance;\ntests of distribution.\n\n\nThe inferential framework brought to bear on these ‘simpler’ problems largely carried over when the field began to specialize."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "href": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "title": "On projects in(volving) data science",
    "section": "Contrasting approaches",
    "text": "Contrasting approaches\nFrom 1960-2010, adopters of the ‘data analysis as a field’ view were largely industry practitioners and applied statisticians who advocated for training and practice that included empirical methods and computation in addition to statistical inference (Donoho 2017).\n\nTheir ideas evolved into an alternative approach to working with data:\n\ndata-driven rather than theory-driven;\niterative rather than conclusive."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "href": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "title": "On projects in(volving) data science",
    "section": "Confirmatory approach",
    "text": "Confirmatory approach\nThe “confirmatory” approach of the classical inferential framework.\n\n\n\n\n\n\n\nconfirm\n\n \n\ncluster_1\n\n data generation  \n\ncluster_2\n\n data analysis  \n\ncluster_3\n\n decision   \n\nsci\n\n domain  knowledge   \n\nhyp\n\n hypotheses   \n\nsci->hyp\n\n    \n\nexp\n\n designed  experiment   \n\nhyp->exp\n\n    \n\nmdl\n\n statistical  model   \n\nexp->mdl\n\n    \n\ndat\n\n data   \n\nexp->dat\n\n    \n\nyay\n\n supporting  evidence   \n\nmdl->yay\n\n    \n\nnay\n\n opposing  evidence   \n\nmdl->nay\n\n    \n\ndat->yay\n\n    \n\ndat->nay\n\n   \n\n\n\n\n\n\noutput is a decision\nstatistical model determined by experimental design\nanalysis based on statistical theory"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#exploratory-approach",
    "href": "materials/slides/week1-perspectives.html#exploratory-approach",
    "title": "On projects in(volving) data science",
    "section": "Exploratory approach",
    "text": "Exploratory approach\nThe “exploratory” approach of iterative modern data analysis.\n\n\n\n\n\n\n\nexplore\n\n \n\ncluster_1\n\n data analysis  \n\ncluster_2\n\n findings   \n\nsci\n\n domain  knowledge   \n\nq\n\n question  formulation   \n\nsci->q\n\n    \n\ndat\n\n data   \n\nq->dat\n\n    \n\nmdl\n\n statistical  model   \n\ndat->mdl\n\n    \n\nmdl->q\n\n    \n\nf1\n\n finding 1   \n\nmdl->f1\n\n    \n\nf2\n\n finding 2   \n\nmdl->f2\n\n    \n\ndots\n\n ⋮  \n\n\n\n\n\n\noutputs are findings\nstatistical model determined by data\nanalysis techniques include empirical methods"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#drivers-of-change",
    "href": "materials/slides/week1-perspectives.html#drivers-of-change",
    "title": "On projects in(volving) data science",
    "section": "Drivers of change",
    "text": "Drivers of change\nIn the 2000s and especially after 2010, the iterative approach enjoys broader applicability than it used to:\n\ndue to automated and/or scalable data collection\n\nobservational data is widely available across domains\nand includes large numbers of variables\n\nhighly specialized data problems evade methodology with theoretical support\nmore accessible to analysts without advanced statistical training"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#machine-learning",
    "href": "materials/slides/week1-perspectives.html#machine-learning",
    "title": "On projects in(volving) data science",
    "section": "Machine learning",
    "text": "Machine learning\nMachine learning was largely advanced by computer scientists through 2010 and later (Emmert-Streib et al. 2020), most notably:\n\nneural networks and deep learning\noptimization\nalgorithmic analysis\n\n\nThis was a major driver in advancing modern predictive modeling, and engaging with these tools required going beyond statistics."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "href": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "title": "On projects in(volving) data science",
    "section": "A theory about data science",
    "text": "A theory about data science\n\nAround mid-century, it was proposed that specialists should be trained in computational as well as statistical methods\nOver time practitioners developed iterative processes for data-driven problem solving that was more flexible than the classical inferential framework\nComputer scientists advanced the field of machine learning substantially\nIterative problem solving together with applied machine learning was well-suited to meet the demands of modern data, but the area was not codified in an academic discipline"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#what-is-research",
    "href": "materials/slides/week1-perspectives.html#what-is-research",
    "title": "On projects in(volving) data science",
    "section": "What is research?",
    "text": "What is research?\nResearch is systematic investigation undertaken in order to establish or discover facts.\n\nWhat are facts in data science?\n\nmethod M outperforms method M’ at task T\nwe analyzed data D and reached the conclusion that…"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#the-research-landscape",
    "href": "materials/slides/week1-perspectives.html#the-research-landscape",
    "title": "On projects in(volving) data science",
    "section": "The research landscape",
    "text": "The research landscape\nFormal communities – i.e., journals, departments, conferences – have not coalesced around data science research to date.\n\nRelevant research largely occurs in statistics, computer science, and application domains, and can be divided broadly into:\n\nmethodology – creating new techniques to analyze data\napplications – applying existing methods to generate new findings"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#methodological-research",
    "href": "materials/slides/week1-perspectives.html#methodological-research",
    "title": "On projects in(volving) data science",
    "section": "Methodological research",
    "text": "Methodological research\nMethodological research might involve:\n\ndesigning a faster algorithm for solving a particular problem\nproposing a new technique for analyzing a particular type of data\ngeneralizing a technique to a broader range of problems"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#applied-research",
    "href": "materials/slides/week1-perspectives.html#applied-research",
    "title": "On projects in(volving) data science",
    "section": "Applied research",
    "text": "Applied research\nApplied research might involve:\n\nanalyzing a specific dataset or producing a novel analysis of existing data\ncreating ad-hoc methods for a domain-specific problem\nimporting methodology from another area to bear on a domain-specific problem"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#data-science-capstones",
    "href": "materials/slides/week1-perspectives.html#data-science-capstones",
    "title": "On projects in(volving) data science",
    "section": "Data science capstones",
    "text": "Data science capstones\nMost of the time, our data science capstones fall pretty squarely in the applied domain:\n\nsponsor provides data and high-level goals\nstudent team works on producing an analysis or analyses\nmentor advises on methodology"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#relevant-skills",
    "href": "materials/slides/week1-perspectives.html#relevant-skills",
    "title": "On projects in(volving) data science",
    "section": "Relevant skills",
    "text": "Relevant skills\nThere are a few avenues to prepare for this sort of work.\n\nWe’ll focus on:\n\nrecognizing problem patterns\ndeveloping a functional view of methodology\ncollaborating efficiently\nindependent learning strategies\nengaging with literature constructively\n\n\n\nIt won’t provide you with exhaustive methodological preparation, but should support you in learning ‘on the job’."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#reading-responses",
    "href": "materials/slides/week1-perspectives.html#reading-responses",
    "title": "On projects in(volving) data science",
    "section": "Reading responses",
    "text": "Reading responses\nQuestions on the perspectives paper (Peng and Parker 2022) to review:\n\nWhat is meant by a ‘systems approach’ to data science?\nWhat is meant by ‘design thinking’ in data science?\n(Why) Are these useful concepts?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#systems-approach",
    "href": "materials/slides/week1-perspectives.html#systems-approach",
    "title": "On projects in(volving) data science",
    "section": "Systems approach",
    "text": "Systems approach\n\n\n\n\nSeveral systems affect the relationship between expected and actual results. Where would you locate them on the figure?\n\nData analytic\nSoftware\nScientific"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "href": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "title": "On projects in(volving) data science",
    "section": "Example systems for data cleaning",
    "text": "Example systems for data cleaning\n\n\nHow might this diagram help an analyst?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-thinking",
    "href": "materials/slides/week1-perspectives.html#design-thinking",
    "title": "On projects in(volving) data science",
    "section": "Design thinking",
    "text": "Design thinking\nThe design thinking framework might be summed up:\n\ndata scientists trade in data analyses\na data analysis is a designed product\nthinking about design principles can help make a better product\n\n\nMany of you focused on how design principles are a response to project constraints. Are there other ways a design perspective might be useful?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-1",
    "href": "materials/slides/week1-perspectives.html#scenario-1",
    "title": "On projects in(volving) data science",
    "section": "Scenario 1",
    "text": "Scenario 1\nYou’re working at a news organization and developing a recommender system for targeted article previews to deploy on the organization’s website. It will show users article previews based on their behavior. Assume you don’t have any significant resource constraints, and can access users’ profiles in full and log interactions in near-real-time.\n\nGoal: show previews most likely to attract interest.\n\n\nConsiderations:\n\nwhat material should be shown in the preview? headlines? images? text?\nwhat behavior can/should be leveraged for the recommender system?\nwhat are a few relevant design aspects of how the system should behave?\nare there ethical concerns?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-2",
    "href": "materials/slides/week1-perspectives.html#scenario-2",
    "title": "On projects in(volving) data science",
    "section": "Scenario 2",
    "text": "Scenario 2\nYou’re working on a research team studying ecological impacts of land use. The team has access to longitudinal species surveys at locations of interest across the U.S., quarterly county-level land allocation statistics, satellite images, and state budget information for sustainability, restoration, and conservation initiatives.\n\nGoal: identify intervention opportunities that are most likely to positively impact ecological diversity.\n\n\nConsiderations:\n\nwhat data would you use and how would you combine data sources?\nare there external data that might be useful?\nwhat analysis outputs would be most important for identifying intervention opportunities?\ncan you think of other design features that might be useful for the data analysis?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "href": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "title": "On projects in(volving) data science",
    "section": "A few design principles",
    "text": "A few design principles\nLet’s look at some design principles from (McGowan, Peng, and Hicks 2021)."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "href": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "title": "On projects in(volving) data science",
    "section": "Design principles: matchedness",
    "text": "Design principles: matchedness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "href": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "title": "On projects in(volving) data science",
    "section": "Design principles: exhuastiveness",
    "text": "Design principles: exhuastiveness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "href": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "title": "On projects in(volving) data science",
    "section": "Design principles: transparency",
    "text": "Design principles: transparency"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "href": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "title": "On projects in(volving) data science",
    "section": "Design principles: reproducibility",
    "text": "Design principles: reproducibility"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#next-time",
    "href": "materials/slides/week1-perspectives.html#next-time",
    "title": "On projects in(volving) data science",
    "section": "Next time",
    "text": "Next time\nWe’ll do a github icebreaker activity.\n\nComplete lab activity from Wednesday section meeting\nBring laptops"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#references",
    "href": "materials/slides/week1-perspectives.html#references",
    "title": "On projects in(volving) data science",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66.\n\n\nEmmert-Streib, Frank, Zhen Yang, Han Feng, Shailesh Tripathi, and Matthias Dehmer. 2020. “An Introductory Review of Deep Learning for Prediction Models with Big Data.” Frontiers in Artificial Intelligence 3: 4.\n\n\nMcGowan, Lucy D’Agostino, Roger D Peng, and Stephanie C Hicks. 2021. “Design Principles for Data Analysis.” arXiv Preprint arXiv:2103.05689.\n\n\nPeng, Roger D, and Hilary S Parker. 2022. “Perspective on Data Science.” Annual Review of Statistics and Its Application 9: 1–20.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nTukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#lessons-from-last-time",
    "href": "materials/slides/week2-classdata.html#lessons-from-last-time",
    "title": "Descriptive analysis of class survey data",
    "section": "Lessons from last time",
    "text": "Lessons from last time\n\nadd .DS_Store to .gitignore\nopen repo project in RStudio session (not another project or new session)\nrepo clone directory must be kept intact; can move the entire directory but not individual files\nuse client not terminal, at least to start out\nothers?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#today",
    "href": "materials/slides/week2-classdata.html#today",
    "title": "Descriptive analysis of class survey data",
    "section": "Today",
    "text": "Today\n\nreview sampling concepts\nintroduce class survey data\npresent descriptive analysis"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#samples-and-populations",
    "href": "materials/slides/week2-classdata.html#samples-and-populations",
    "title": "Descriptive analysis of class survey data",
    "section": "Samples and populations",
    "text": "Samples and populations\npopulation: collection of all subjects/units of interest\nsample: subjects/units observed in a study\n\nstatistical methodology strives to account for the possibility that the sample could have been different in order to make reliable inferences about the population based on knowledge of the sampling mechanism"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "href": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "title": "Descriptive analysis of class survey data",
    "section": "What if inferences aren’t possible?",
    "text": "What if inferences aren’t possible?\nEven if inference isn’t possible, data still have value and could be used for:\n\ndescriptive analysis of the sample;\nhypothesis generation;\ndeveloping analysis pipelines."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-about-prediction",
    "href": "materials/slides/week2-classdata.html#what-about-prediction",
    "title": "Descriptive analysis of class survey data",
    "section": "What about prediction?",
    "text": "What about prediction?\nPrediction is a separate goal but still a form of generalization.\n\nsamples must reflect a broader population for predictions to be accurate at the population level\n\n\n\nif an analyst can’t expect sample statistics to provide reliable estimates of population quantities, they shouldn’t expect predictions based on the sample to be reliable either"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#common-problems",
    "href": "materials/slides/week2-classdata.html#common-problems",
    "title": "Descriptive analysis of class survey data",
    "section": "Common problems",
    "text": "Common problems\nSeveral issues arise very often in practice that compromise or complicate an analyst’s ability to make inferences (or predictions). Among them:\n\nscope of inference from the sample doesn’t match the study population\nsubjects/units are selected haphazardly or by convenience\nresearcher conflates sample size with number of observations, i.e., takes lots of measurements on few subjects/units"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#helpful-questions",
    "href": "materials/slides/week2-classdata.html#helpful-questions",
    "title": "Descriptive analysis of class survey data",
    "section": "Helpful questions",
    "text": "Helpful questions\nThe following questions can help make an assessment of the scope of inference:\n\n(protocol) how were subjects/units chosen for measurement and how were measurements collected?\n(mechanism) was there any random selection mechanism?\n(exclusion) are there any subjects/units that couldn’t possibly have been chosen?\n(nonresponse) were any subjects/units selected but not measured?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#class-survey-data",
    "href": "materials/slides/week2-classdata.html#class-survey-data",
    "title": "Descriptive analysis of class survey data",
    "section": "Class survey data",
    "text": "Class survey data\n\nsurvey distributed to all students offered enrollment in PSTAT197A fall 2022\n\\(n = 65\\) responses\n\nincludes a few students who did not enroll\ndoes not include several students who did not enroll\ndoes not include one student who enrolled late\n\nno random selection"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "href": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "title": "Descriptive analysis of class survey data",
    "section": "Can the data support inference?",
    "text": "Can the data support inference?\nFrom the reading responses:\n\nIt depends on the question. If you want to draw conclusions about the pstat197a class specifically, this sample is the population and thus will have reliable data. If you want to draw conclusions about the pstat department as a whole, then this is a bad sample because it is likely biased and thus unreliable"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#alternative-perspectives",
    "href": "materials/slides/week2-classdata.html#alternative-perspectives",
    "title": "Descriptive analysis of class survey data",
    "section": "Alternative perspectives",
    "text": "Alternative perspectives\nThe comment points to two ways to view the data:\n\na census of PSTAT197A enrollees\na convenience sample of…\n\ncapstone applicants OR\nstudents qualified for capstones OR\nstudents interested in data science OR\nall UCSB students???"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "href": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "title": "Descriptive analysis of class survey data",
    "section": "Is there a right answer?",
    "text": "Is there a right answer?\nEither way – census or convenience sample – excludes inference.\n\ncensus \\(\\longrightarrow\\) no inference needed\nconvenience \\(\\longrightarrow\\) no inference possible\n\n\nSo on a practical level, it won’t make much difference for designing an analysis of the survey data."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#descriptive-analysis",
    "href": "materials/slides/week2-classdata.html#descriptive-analysis",
    "title": "Descriptive analysis of class survey data",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nAny analysis of survey data should be regarded as descriptive in nature:\n\nsummary statistics and/or models are not reliable measures of any broader population\nresults should be interpreted narrowly in terms of the sample at hand"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#a-general-approach",
    "href": "materials/slides/week2-classdata.html#a-general-approach",
    "title": "Descriptive analysis of class survey data",
    "section": "A general approach",
    "text": "A general approach\nStart simple and add complexity gradually.\nFrom simpler to more complex consider questions involving:\n\nSample characteristics\nSingle-variable summaries\nMultivariate summaries\nModel-based outputs (estimates, predictions, etc.)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest",
    "href": "materials/slides/week2-classdata.html#questions-of-interest",
    "title": "Descriptive analysis of class survey data",
    "section": "Questions of interest",
    "text": "Questions of interest\nSample characteristics\n\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nSingle-variable summaries\n\nAmong the students offered a seat in PSTAT197, what fields of study are the students most interested in?\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "href": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "title": "Descriptive analysis of class survey data",
    "section": "Questions of interest",
    "text": "Questions of interest\nMultivariate summaries\n\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nModel-based outputs\n\nAre there distinct groups of students in the class defined by self-assessed proficiencies and/or comfort levels with mathematics, statistics, and programming?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#sample-characteristics",
    "href": "materials/slides/week2-classdata.html#sample-characteristics",
    "title": "Descriptive analysis of class survey data",
    "section": "Sample characteristics",
    "text": "Sample characteristics\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\n\n\n\nClass standingGenderRaceData sharing\n\n\n\n\n\n\n\nstanding\nn\n\n\n\n\nJunior\n9\n\n\nSenior\n56\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nn\n\n\n\n\nFemale\n25\n\n\nMale\n40\n\n\n\n\n\n\n\n\n\n\n\n\nrace\nn\n\n\n\n\nAsian\n41\n\n\nCaucasian\n17\n\n\nPrefer not to say\n6\n\n\nUnknown\n1\n\n\n\n\n\n\n\nColumns: consent to share project preferences\nRows: consent to share background and preparation\n\n\n\n\n\n\nNo\nYes\n\n\n\n\nNo\n3\n3\n\n\nYes\n2\n57"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#majors",
    "href": "materials/slides/week2-classdata.html#majors",
    "title": "Descriptive analysis of class survey data",
    "section": "Majors",
    "text": "Majors"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#response-timing",
    "href": "materials/slides/week2-classdata.html#response-timing",
    "title": "Descriptive analysis of class survey data",
    "section": "Response timing",
    "text": "Response timing"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#privacy",
    "href": "materials/slides/week2-classdata.html#privacy",
    "title": "Descriptive analysis of class survey data",
    "section": "Privacy",
    "text": "Privacy\nThe following information have been removed from the dataset distributed to the class:\n\npersonal information from section 1 of the survey\nlong text and free response answers, contain some personal details\nresponses from students who did not consent to share\ntype distinction between research experiences"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#single-variable-summaries",
    "href": "materials/slides/week2-classdata.html#single-variable-summaries",
    "title": "Descriptive analysis of class survey data",
    "section": "Single-variable summaries",
    "text": "Single-variable summaries\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?\n\nComfortProficiency (numeric)Proficiency (factor)\n\n\n\n\n\n\n\nvariable\nmax\nmean\nmedian\nmin\n\n\n\n\nmath.comf\n5\n3.847458\n4\n2\n\n\nprog.comf\n5\n3.966102\n4\n3\n\n\nstat.comf\n5\n4.084746\n4\n2\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\n\n\n\n\nmath\n2.355932\n2\n\n\nprog\n2.237288\n2\n\n\nstat\n2.576271\n3\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nn1\nmath\nn2\nstat\nn3\n\n\n\n\nBeg\n3\nBeg\n3\nBeg\n2\n\n\nInt\n39\nInt\n32\nInt\n21\n\n\nAdv\n17\nAdv\n24\nAdv\n36"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#multivariable-summaries",
    "href": "materials/slides/week2-classdata.html#multivariable-summaries",
    "title": "Descriptive analysis of class survey data",
    "section": "Multivariable summaries",
    "text": "Multivariable summaries\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nCountsProportions\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\n\n\n\n\n[1,2.33]\n6\n25\n1\n\n\n(2.33,2.67]\n5\n9\n1\n\n\n(2.67,3]\n3\n6\n1\n\n\n\n\n\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\nn\n\n\n\n\n[1,2.33]\n0.188\n0.781\n0.031\n32\n\n\n(2.33,2.67]\n0.333\n0.600\n0.067\n15\n\n\n(2.67,3]\n0.300\n0.600\n0.100\n10"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#combinations",
    "href": "materials/slides/week2-classdata.html#combinations",
    "title": "Descriptive analysis of class survey data",
    "section": "Combinations",
    "text": "Combinations\nConsider the distinct combinations of comfort and proficiency ratings (separately):\n\nProficiencyComfort\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n1\n1\n1\n1\n\n\n1\n2\n2\n1\n\n\n1\n2\n3\n1\n\n\n2\n1\n1\n1\n\n\n2\n1\n2\n1\n\n\n2\n2\n2\n13\n\n\n2\n2\n3\n11\n\n\n2\n3\n2\n3\n\n\n2\n3\n3\n10\n\n\n3\n2\n2\n2\n\n\n3\n2\n3\n4\n\n\n3\n3\n2\n1\n\n\n3\n3\n3\n10\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n3\n2\n2\n1\n\n\n3\n3\n3\n2\n\n\n3\n3\n4\n2\n\n\n3\n3\n5\n2\n\n\n3\n4\n3\n4\n\n\n3\n4\n4\n7\n\n\n4\n3\n3\n2\n\n\n4\n3\n4\n5\n\n\n4\n3\n5\n2\n\n\n4\n4\n3\n1\n\n\n4\n4\n4\n5\n\n\n4\n4\n5\n3\n\n\n4\n5\n3\n1\n\n\n4\n5\n4\n4\n\n\n4\n5\n5\n2\n\n\n5\n3\n4\n5\n\n\n5\n3\n5\n1\n\n\n5\n4\n4\n2\n\n\n5\n4\n5\n1\n\n\n5\n5\n4\n1\n\n\n5\n5\n5\n6"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#clustering",
    "href": "materials/slides/week2-classdata.html#clustering",
    "title": "Descriptive analysis of class survey data",
    "section": "Clustering",
    "text": "Clustering\nCan students be grouped based on combinations of preferences and comfort levels?\n\nCentersVisualizationMethodInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprog.prof\nmath.prof\nstat.prof\nprog.comf\nmath.comf\nstat.comf\nsize\ncluster\n\n\n\n\n2.478\n2.739\n2.957\n4.435\n4.522\n4.522\n23\n1\n\n\n2.048\n1.857\n2.238\n4.048\n3.048\n4.048\n21\n2\n\n\n2.133\n2.467\n2.467\n3.133\n3.933\n3.467\n15\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering method, “k means”, groups data by nearest Euclidean distance to each of \\(k\\) centers. \\(k\\) is user-specified; the method finds the centers that minimize within-cluster variance.\n\n\nBased on the centers:\n\nCluster 1: advanced proficiency, very comfortable\nCluster 2: intermediate with less mathematical preparation\nCluster 3: intermediate with less programming preparation"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#assignment",
    "href": "materials/slides/week2-classdata.html#assignment",
    "title": "Descriptive analysis of class survey data",
    "section": "Assignment",
    "text": "Assignment\nYour task is to extend this analysis with your group by next Tuesday.\n\nHere are some ideas:\n\nexplore variable associations further (e.g., coursework and self-evaluations)\nexperiment with clustering on different variable subsets or using different methods\nsummarize domain or area of interest variables (requires some text manipulation)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#next-time",
    "href": "materials/slides/week2-classdata.html#next-time",
    "title": "Descriptive analysis of class survey data",
    "section": "Next time",
    "text": "Next time\nMost of next meeting we’ll devote to planning your group’s task.\n\nDo a little brainstorming on your own\nCome with a few questions/ideas"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#today",
    "href": "materials/slides/week2-workshop.html#today",
    "title": "Group assignment workshops",
    "section": "Today",
    "text": "Today\n\nfinish discussing results from last time\nsetup for first group assignment due next Friday\n\nassignment objectives and instructions\nreview repository\n\nworkshop ideas and plan tasks in groups"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-objective",
    "href": "materials/slides/week2-workshop.html#assignment-objective",
    "title": "Group assignment workshops",
    "section": "Assignment objective",
    "text": "Assignment objective\nYour task: prepare and present a descriptive analysis of the survey responses addressing 2-3 questions or goals of your choosing.\n\nquestions or goals should be of moderate complexity; easy to state and understand but should require a little work to answer\n\ntoo simple: what proportion of students have research experience?\nbetter: are students with research experience more confident/comfortable with technical skills than students without, considering coursework history?"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#outcomes",
    "href": "materials/slides/week2-workshop.html#outcomes",
    "title": "Group assignment workshops",
    "section": "Outcomes",
    "text": "Outcomes\nThe learning outcomes for this assignment are less focused on methodology:\n\nformulate questions and plan a simple analysis\npractice using a GitHub repo in a team project setting\nprepare a report\nlearn one or more new-to-you data manipulation techniques"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-set-up",
    "href": "materials/slides/week2-workshop.html#assignment-set-up",
    "title": "Group assignment workshops",
    "section": "Assignment set-up",
    "text": "Assignment set-up\n\naccept GH classroom assignment here; this will create/add your team repo\nclone repo to local\nreview repo contents:\n\ndata with survey responses and metadata\nscripts with preprocessing and in-class analysis\nresults with report template for preparing write-up\nREADME.md with assignment instructions"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#deliverable",
    "href": "materials/slides/week2-workshop.html#deliverable",
    "title": "Group assignment workshops",
    "section": "Deliverable",
    "text": "Deliverable\nYour ‘submission’ will be in the form of commits to the group repository, in particular:\n\nan updated results/report.qmd file containing your write-up source\na rendered results/report.html file\n\n\nPlease make final commits by Friday, October 14, 11:59pm PST.\n\n\nRecall there is a 24-hour grace period; any commits submitted after Saturday 11:59pm may not receive review."
  },
  {
    "objectID": "materials/slides/week2-workshop.html#resources",
    "href": "materials/slides/week2-workshop.html#resources",
    "title": "Group assignment workshops",
    "section": "Resources",
    "text": "Resources\n\nlab 2 and in-class analysis scripts/week2-surveys.R\nslack and course staff OH\nMDSR ch. 4 on data wrangling\nRStudio cheatsheets and tidyverse documentation, esp. dplyr, tidyr, ggplot2"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#workshopping-today",
    "href": "materials/slides/week2-workshop.html#workshopping-today",
    "title": "Group assignment workshops",
    "section": "Workshopping today",
    "text": "Workshopping today\nTry to accomplish three goals:\n\npool ideas for questions or themes to explore in the data\npair up, divide work, and assign tasks\nagree on a communication plan for finishing work\n\nslack groupchat or similar\nmeeting outside of class sometime next week"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "href": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "title": "Biomarkers of ASD",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ndon’t forget to fill out attendance form for each class meeting\n\nbut don’t fill it out if you don’t come to class\n\nfirst group assignment due Friday 10/14 11:59pm PST\n\nplease add your lab scripts from labs 1, 2\nlabs/labN-TITLE-USERNAME.R\n\nsection attendance is expected"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#background",
    "href": "materials/slides/week3-biomarkers.html#background",
    "title": "Biomarkers of ASD",
    "section": "Background",
    "text": "Background\nLevels of proteins in plasma/serum are altered in autism spectrum disorder (ASD).\n\nGoal: identify a panel of proteins useful as a blood biomarker for early detection of ASD.\n\na ‘panel’ is a handful of tests that help distinguish between conditions\nso in other words, find proteins whose serum levels are predictive of ASD"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#dataset",
    "href": "materials/slides/week3-biomarkers.html#dataset",
    "title": "Biomarkers of ASD",
    "section": "Dataset",
    "text": "Dataset\nData from Hewitson et al. (2021)\n\nSerum samples from 76 boys with ASD and 78 typically developing (TD) boys, 18 months-8 years of age\nA total of 1,125 proteins were analyzed from each sample\n\n1,317 measured, 192 failed quality control\n(we don’t know which ones failed QC so will use all)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "href": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "title": "Biomarkers of ASD",
    "section": "Sample characteristics",
    "text": "Sample characteristics\n\nAgeDemographicsComorbiditiesMedications\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nAge: mean (SD) years\n5.6 (1.7)\n5.7 (2.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nWhite/Caucasian\n33 (45.2%)\n40 (51.9%)\n\n\nHispanic/Latino\n26 (35.6%)\n6 (7.8%)\n\n\nAfrican American/Black\n3 (4.1%)\n14 (18.2%)\n\n\nAsian or Pacific Islander\n2 (2.6%)\n3 (3.9%)\n\n\nMultiple ethnicities or Other\n9 (12.3%)\n14 (18.2%)\n\n\nNot reported\n3 (4.1%)\n1 (1.2%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n38 (52.8%)\n58 (75.3%)\n\n\nADHD\n2 (2.8%)\n1 (1.3%)\n\n\nSeasonal Allergies\n30 (41.7%)\n17 (22.4%)\n\n\nAsthma\n2 (2.8%)\n0 (0%)\n\n\nCeliac Disease\n1 (1.4%)\n0 (0%)\n\n\nGERD\n1 (1.4%)\n0 (0%)\n\n\nPTSD\n0 (0%)\n1 (1.3%)\n\n\nSleep Apnea\n2 (2.8%)\n0 (0%)\n\n\nNot reported\n4 (5.6%)\n1 (1.3%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n69 (92%)\n75 (97.4%)\n\n\nAnti-depressant\n2 (2.7%)\n0 (0%)\n\n\nAnti-psychotic\n0 (0%)\n1 (1.3%)\n\n\nSedative\n1 (1.3%)\n0 (0%)\n\n\nSSRI\n2 (2.27%)\n0 (0%)\n\n\nStimulant\n1 (1.3%)\n1 (1.3%)\n\n\nNot reported\n1 (1.3%)\n1 (1.3%)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#data-glimpse",
    "href": "materials/slides/week3-biomarkers.html#data-glimpse",
    "title": "Biomarkers of ASD",
    "section": "Data glimpse",
    "text": "Data glimpse\n\nExample rowsGroup sizes\n\n\n\nasd_clean %>% head(5)\n\n# A tibble: 5 × 1,318\n  group    CHIP  CEBPB     NSE   PIAS4 `IL-10 Ra`  STAT3   IRF1 `c-Jun` `Mcl-1`\n  <chr>   <dbl>  <dbl>   <dbl>   <dbl>      <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n1 ASD    0.335   0.520 -0.554   0.650      -0.358  0.305 -0.484   0.309  1.57  \n2 ASD   -0.0715  1.01   3       1.28       -0.133  1.13   0.253   0.408  0.0643\n3 ASD   -0.406  -0.531 -0.0592  1.13        0.554 -0.334  0.287  -0.845  1.42  \n4 ASD   -0.102  -0.251  1.47    0.0773     -0.705  0.893  2.61   -0.372 -0.467 \n5 ASD   -0.395  -0.536  0.0410 -0.299      -0.830  0.899  1.01   -0.843 -1.15  \n# … with 1,308 more variables: OAS1 <dbl>, `c-Myc` <dbl>, SMAD3 <dbl>,\n#   SMAD2 <dbl>, `IL-23` <dbl>, PDGFRA <dbl>, `IL-12` <dbl>, STAT1 <dbl>,\n#   STAT6 <dbl>, LRRK2 <dbl>, Osteocalcin <dbl>, `IL-5` <dbl>, GPDA <dbl>,\n#   IgA <dbl>, LPPL <dbl>, HEMK2 <dbl>, PDXK <dbl>, TLR4 <dbl>, REG4 <dbl>,\n#   `HSP 27` <dbl>, `YKL-40` <dbl>, `Alpha enolase` <dbl>, `Apo L1` <dbl>,\n#   CD38 <dbl>, CD59 <dbl>, FABPL <dbl>, `GDF-11` <dbl>, BTC <dbl>,\n#   `HIF-1a` <dbl>, S100A6 <dbl>, SECTM1 <dbl>, RSPO3 <dbl>, PSP <dbl>, …\n\n\n\n\n\n\n# A tibble: 2 × 2\n  group     n\n  <chr> <int>\n1 ASD      76\n2 TD       78"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#module-objectives",
    "href": "materials/slides/week3-biomarkers.html#module-objectives",
    "title": "Biomarkers of ASD",
    "section": "Module objectives",
    "text": "Module objectives\nMethodology\n\nmultiple testing\nclassification: logistic regression; random forests\nvariable selection: LASSO regularization\nclassification accuracy measures\n\n\nConcepts\n\ndata partitioning for predictive modeling\nmodel interpretability\nhigh dimensional data \\(n < p\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#marginal-differences",
    "href": "materials/slides/week3-biomarkers.html#marginal-differences",
    "title": "Biomarkers of ASD",
    "section": "Marginal differences",
    "text": "Marginal differences\nIdea: test for a significant difference in serum levels between groups for a given protein, say protein \\(i\\).\n\nNotation:\n\n\\(\\mu^i_{ASD}\\): mean serum level of protein \\(i\\) in the ASD group\n\\(\\mu^i_{TD}\\): mean serum level of protein \\(i\\) in the TD group\n\\(\\delta_i\\): difference in means \\(\\mu^i_{ASD} - \\mu^i_{TD}\\)\nhats indicate sample estimates (e.g. \\(\\hat{\\delta}_i\\))"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-t-test",
    "href": "materials/slides/week3-biomarkers.html#review-t-test",
    "title": "Biomarkers of ASD",
    "section": "Review: \\(t\\)-test",
    "text": "Review: \\(t\\)-test\nThe \\(t\\)-test tests \\(H_{0i}: \\delta_i = 0\\) against its negation \\(\\neg H_{0i}: \\delta_i \\neq 0\\) using the rule\n\\[\n\\text{reject $H_{0i}$ if}\\qquad \\left|\\frac{\\hat{\\delta}_i}{SE(\\hat{\\delta}_i)}\\right| > t_\\alpha\n\\]\n\n\\(SE(\\hat{\\delta}_i)\\) is a standard error for the difference estimate; quantifies variability of the estimate\nprocedure controls type I error at \\(\\alpha\\), ensuring \\(P\\left(\\text{reject}_i|H_i\\right) \\leq 0.05\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-p-values",
    "href": "materials/slides/week3-biomarkers.html#review-p-values",
    "title": "Biomarkers of ASD",
    "section": "Review: \\(p\\)-values",
    "text": "Review: \\(p\\)-values\nThe \\(p\\)-value for a test is the probability of obtaining a sample at least as contrary to \\(H_{0i}\\) as the sample in hand, assuming \\(H_{0i}\\) is true.\n\nBy construction, \\(p < \\alpha\\) just in case the test rejects with type I error controlled at \\(\\alpha\\).\n\n\nSo a common heuristic is:\n\\[\n\\text{reject $H_{0i}$ if} \\qquad p_i \\leq \\alpha\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#one-test",
    "href": "materials/slides/week3-biomarkers.html#one-test",
    "title": "Biomarkers of ASD",
    "section": "One test",
    "text": "One test\nHere is R output for one test.\n\nasd %>%\n  t_test(formula = CHIP ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1     0.927  75.7   0.357 two.sided       384.    -441.    1210.\n\n\n\nQuestions:\n\nWhat are the hypotheses in words?\nWhat are the test assumptions?\nWhat is the conclusion of the test?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#many-tests",
    "href": "materials/slides/week3-biomarkers.html#many-tests",
    "title": "Biomarkers of ASD",
    "section": "Many tests",
    "text": "Many tests\nA plausible approach for identifying a protein panel, then, is to select all those proteins for which the \\(t\\)-test indicates a significant difference.\n\n1,317 tests\neasy to compute\nconceptually straightforward\n\n\nHow likely are mistakes?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#test-outcomes",
    "href": "materials/slides/week3-biomarkers.html#test-outcomes",
    "title": "Biomarkers of ASD",
    "section": "Test outcomes",
    "text": "Test outcomes\nLet \\(H_i\\) denote the \\(i\\)th null hypothesis and \\(R_i\\) denote the event that \\(H_i\\) is rejected.\n\n\n\n\n\n\\(H_i\\)\n\\(\\neg H_i\\)\n\n\n\n\n\\(R_i\\)\n\\(V\\) false rejections\n\\(S\\) correct\n\n\n\\(\\neg R_i\\)\n\\(T\\) correct\n\\(W\\) false non-rejections\n\n\n\n\n\nThe multiple testing problem is that individual error rates compound over multiple tests."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#familywise-error",
    "href": "materials/slides/week3-biomarkers.html#familywise-error",
    "title": "Biomarkers of ASD",
    "section": "Familywise error",
    "text": "Familywise error\nFamilywise error rate (FWER) is the probability of one or more type I errors: \\(P(V \\geq 1)\\).\n\nSuppose there are \\(m\\) true hypotheses \\(\\mathcal{H}: \\{H_i: i \\in C\\}\\).\n\n\nIf the tests are independent and exact then:\n\\[\n\\begin{aligned}\nP(V \\geq 1)\n&= P\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\\\\n&= 1 - \\prod_{i \\in C} \\left( 1- P(R_i|H_i) \\right) \\\\\n&= 1 - (1 - \\alpha)^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#fwer-example",
    "href": "materials/slides/week3-biomarkers.html#fwer-example",
    "title": "Biomarkers of ASD",
    "section": "FWER Example",
    "text": "FWER Example\nIf individual tests are exactly controlled at \\(\\alpha = 0.05\\) and independent, at least one error is nearly certain by 100 tests.\n\nFamilywise error rate as a function of the number of tests, assuming tests are independent with exact type I error 0.05."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "href": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "title": "Biomarkers of ASD",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nThe simplest multiple testing correction is based on the Bonferroni inequality:\n\\[\nP\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\leq \\sum_{i \\in C} P(R_i|\\mathcal{H})\n\\]\n\nIf the individual tests are controlled at level \\(\\alpha\\), then \\(FWER \\leq m\\alpha\\).\n\n\nSo a simple solution is to test at level \\(\\alpha^* = \\frac{\\alpha}{m}\\).\n\n\nIn other words, reject if \\(p_i < \\frac{\\alpha}{m}\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "href": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "title": "Biomarkers of ASD",
    "section": "False discovery rate",
    "text": "False discovery rate\nFWER control will limit false rejections, but at the cost of power; controlling the probability of one type I error is a conservative approach.\n\nMore common in modern applications are procedures to control false discovery rate: the expected proportion of rejections that are false.\n\\[\n\\text{FDR} = \\mathbb{E}\\left[\\frac{\\text{false rejections}}{\\text{total rejections}}\\right]\n\\]\n\n\nConceptually, if say FDR is controlled at \\(0.05\\), then one would expect 5% of rejections to be false."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "title": "Biomarkers of ASD",
    "section": "Benjamini-Hochberg correction",
    "text": "Benjamini-Hochberg correction\nBenjamini and Hochberg (1995) conceived a procedure based on sorting \\(p\\)-values.\n\nSupposing \\(m\\) independent tests are performed:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} < \\frac{i\\alpha}{m}\\)\n\n\n\nThey proved that this controls FDR at \\(\\alpha\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "title": "Biomarkers of ASD",
    "section": "Benjamini-Yekutieli correction",
    "text": "Benjamini-Yekutieli correction\nThe Benjamini-Hochberg assumes tests are independent, which is obviously not true in most situations. (Why?)\n\nBenjamini and Yekutieli (2001) modified the correction to hold without the independence assumption:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} < \\frac{i\\alpha}{m H_m}\\)\n\n\n\nAbove, \\(H_m = \\sum_{i = 1}^m \\frac{1}{i}\\) ."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "href": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "title": "Biomarkers of ASD",
    "section": "Implementing corrections",
    "text": "Implementing corrections\nThe easiest way to implement these corrections is to adjust the \\(p\\)-values with a multiplier:\n\n(Bonferroni) \\(p^b_i = m\\times p_i\\)\n(Benjamini-Hochberg) \\(p^{bh}_{(i)} = \\frac{m}{i} p_{(i)}\\)\n(Benjamini-Yekuteili) \\(p^{bh}_{(i)} = \\frac{m H_m}{i} p_{(i)}\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#computations",
    "href": "materials/slides/week3-biomarkers.html#computations",
    "title": "Biomarkers of ASD",
    "section": "Computations",
    "text": "Computations\n\nPreprocessingAssumptionsTestsCorrections\n\n\n\ntrim_fn <- function(x){\n  x[x > 3] <- 3\n  x[x < -3] <- -3\n  \n  return(x)\n}\n\nasd_clean <- asd %>% \n  select(-ados) %>%\n  # log transform\n  mutate(across(.cols = -group, log10)) %>%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %>%\n  # trim outliers (affects results??)\n  mutate(across(.cols = -group, trim_fn))\n\nasd_nested <- asd_clean %>%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %>%\n  nest(data = c(level, group))\n\nasd_nested %>% head(4)\n\n# A tibble: 4 × 2\n  protein data              \n  <chr>   <list>            \n1 CHIP    <tibble [154 × 2]>\n2 CEBPB   <tibble [154 × 2]>\n3 NSE     <tibble [154 × 2]>\n4 PIAS4   <tibble [154 × 2]>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# compute for several groups\ntest_fn <- function(.df){\n  t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\ntt_out <- asd_nested %>%\n  mutate(ttest = map(data, test_fn)) %>%\n  unnest(ttest) %>%\n  arrange(p_value)\n\ntt_out %>% head(5)\n\n# A tibble: 5 × 9\n  protein     data     statistic  t_df     p_value alternative estimate lower_ci\n  <chr>       <list>       <dbl> <dbl>       <dbl> <chr>          <dbl>    <dbl>\n1 DERM        <tibble>     -6.10  151.     8.27e-9 two.sided     -0.885    -1.17\n2 RELT        <tibble>     -5.65  152.     7.82e-8 two.sided     -0.775    -1.05\n3 FSTL1       <tibble>     -5.27  152.     4.66e-7 two.sided     -0.783    -1.08\n4 C1QR1       <tibble>     -5.26  152.     4.79e-7 two.sided     -0.782    -1.08\n5 Calcineurin <tibble>     -5.24  151.     5.37e-7 two.sided     -0.734    -1.01\n# … with 1 more variable: upper_ci <dbl>\n\n\n\n\n\n# multiple testing corrections\nm <- nrow(tt_out)\nhm <- log(m) + 1/(2*m) - digamma(1)\n  \ntt_corrected <- tt_out %>%\n  select(data, protein, p_value) %>%\n  mutate(rank = row_number()) %>%\n  mutate(p_bh = p_value*m/rank,\n         p_by = p_value*m*hm/rank,\n         p_bonf = p_value*m)\n\ntt_corrected %>% head(5)\n\n# A tibble: 5 × 7\n  data               protein           p_value  rank      p_bh      p_by  p_bonf\n  <list>             <chr>               <dbl> <int>     <dbl>     <dbl>   <dbl>\n1 <tibble [154 × 2]> DERM        0.00000000827     1 0.0000109 0.0000845 1.09e-5\n2 <tibble [154 × 2]> RELT        0.0000000782      2 0.0000515 0.000400  1.03e-4\n3 <tibble [154 × 2]> FSTL1       0.000000466       3 0.000205  0.00159   6.14e-4\n4 <tibble [154 × 2]> C1QR1       0.000000479       4 0.000158  0.00122   6.31e-4\n5 <tibble [154 × 2]> Calcineurin 0.000000537       5 0.000141  0.00110   7.07e-4"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#results",
    "href": "materials/slides/week3-biomarkers.html#results",
    "title": "Biomarkers of ASD",
    "section": "Results",
    "text": "Results\n\nComparing methodsTop 10 proteins\n\n\n\n\n\n\n\nAdjusted vs. raw p-values for each multiple correction method.\n\n\n\n\n\n\n\n# top 10\ntt_corrected %>%\n  select(protein, p_by) %>%\n  slice_min(order_by = p_by, n = 10)\n\n# A tibble: 10 × 2\n   protein              p_by\n   <chr>               <dbl>\n 1 DERM            0.0000845\n 2 RELT            0.000400 \n 3 Calcineurin     0.00110  \n 4 C1QR1           0.00122  \n 5 MRC2            0.00132  \n 6 IgD             0.00136  \n 7 CXCL16, soluble 0.00149  \n 8 PTN             0.00154  \n 9 FSTL1           0.00159  \n10 Cadherin-5      0.00179"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "href": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "title": "Biomarkers of ASD",
    "section": "Neat graphic: volcano plot",
    "text": "Neat graphic: volcano plot\n\nUpregulation and downregulation of serum levels of proteins analyzed – p-values against number of doublings (positive) or halvings (negative) of serum level in ASD group relative to TD group."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#next-time",
    "href": "materials/slides/week3-biomarkers.html#next-time",
    "title": "Biomarkers of ASD",
    "section": "Next time",
    "text": "Next time\nOther approaches to the same problem:\n\ncorrelation with ADOS (severity diagnostic score)\nvariable importance in random forest classifier"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#references",
    "href": "materials/slides/week3-biomarkers.html#references",
    "title": "Biomarkers of ASD",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The Control of the False Discovery Rate in Multiple Testing Under Dependency.” Annals of Statistics, 1165–88.\n\n\nHewitson, Laura, Jeremy A Mathews, Morgan Devlin, Claire Schutte, Jeon Lee, and Dwight C German. 2021. “Blood Biomarker Discovery for Autism Spectrum Disorder: A Proteomic Analysis.” PLoS One 16 (2): e0246581."
  }
]