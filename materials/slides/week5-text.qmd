---
title: "Web fraud I"
subtitle: "PSTAT197A/CMPSC190DD Fall 2022"
author: "Trevor Ruiz"
institute: 'UCSB'
bibliography: refs.bib
format: 
  revealjs:
    incremental: true
    # footer: 'PSTAT197A/CMPSC190DD Fall 2022'
    # logo: 'img/ucsbds_hex.png'
    fig-width: 4
    fig-height: 2
    fig-align: 'left'
    slide-number: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

## Announcements/reminders

# Data introduction

## About

The data we'll use for this module comes from a 2021-2022 capstone project.

. . .

***Goal:*** use a predictive model to flag webpages that may contain evidence related to fraud claims.

-   \~ 3K webpages that have been manually labeled according to whether they contain relevant information

-   labels specify type of content (unlawful activity, medical information, fatality, etc.)

-   data comprise URL, raw HTML, and claim label

-   sampling method unclear/unknown, follows some internal protocol at the sponsor's organization

## Data semantics

At face value:

-   The *observations* are webpages; one observation is made per page.

-   The *variables* are claim labels and ... ???

. . .

The first challenge with this data is converting raw HTML into usable quantitative information so that we have some predictor variables to work with.

## Example rows

We will work with a random subsample of 618 observations (pages).

```{r}
#| echo: true
library(tidyverse)
load('data/carpe-raw-subsample.RData')
rawdata %>% head()
```

## Labels

At a high level, this is simply a classification problem.

. . .

Let's consider the labels first (named `internal_feedback`).

```{r}
#| echo: true
# count observations in each category
rawdata %>%  
  count(internal_feedback) %>%
  mutate(prop = n/sum(n)) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```

## Lumping

In practice, it will be extremely difficult to accurately classify labels that only show up \<1% of the time.

. . .

We can ***lump*** factor levels together: combine specific labels that occur infrequently.

```{r}
# combine factor levels
rawdata %>%  
  mutate(class = fct_lump(internal_feedback, prop = 0.05),
         class = fct_infreq(class)) %>%
  count(class) %>% 
  mutate(proportion = n/sum(n)) %>%
  knitr::kable()
```

-   half of pages contain no relevant information

-   the other half are distributed mostly across physical activity, fatality, and unlawful activity

## Binary classification

This is a *multi-class classification* problem.

. . .

If we can't do well with binary classification, there's not much hope for the multi-class setting. So let's start there.

```{r}
# lump and relabel
rawdata_relabeled <- rawdata %>%
  mutate(bclass = fct_lump(internal_feedback, 
                           prop = 0.5, 
                           other_level = 'relevant'),
         bclass = fct_recode(bclass, 
                             irrelevant = 'N/A: No relevant content.'),
         bclass = fct_infreq(bclass))  %>%
  mutate(.id = paste('url', row_number(), sep = '')) %>%
  select(.id, bclass, text_tmp)

rawdata_relabeled %>% head()
```

## Raw HTML  {.scrollable}

Here's what a page looks like.

```{r}
page <- rawdata_relabeled %>% slice(1) %>% pull(text_tmp)
page
```

## Scraping tools  {.scrollable}

As a first step, we need to determine what text information to extract from the page. `rvest` is a new-ish tidyverse package for web scraping.

```{r}
#| echo: true
library(rvest)

# parse
page_text <- read_html(page) %>%
  # extract paragraph elements
  html_elements('p') %>%
  # strip html and extract text
  html_text2()

# print result
page_text
```

## One long string

We can collapse the list into one long character string containing all the paragraph text.

```{r}
string <- page_text %>%
  str_c(collapse = ' ')

string
```

## Extracting words

***Goal:*** whittle down the string to just the words.

. . .

Question. How do you get from this:

```{r}
c('For more information, call @Alfred | (201) 744 5050')
```

To this:

```{r}
c('for more information call alfred')
```

## String manipulation

The `stringr` package contains functions for string manipulation in R, usually starting with the prefix `str_` and often working by pattern matching.

```{r}
#| echo: true

library(stringr)

c('example-string') %>%
str_replace('[[:punct:]]', ' ')
```

. . .

The `qdapRegex` package contains shorthand wrappers for removing common patterns such as emails, URLs, states, etc.

```{r}
library(qdapRegex)

c('email Mildred mildred@mildred.info') %>%
  rm_email() 
```

## Page text processing

Our strategy will be:

1.  Remove URLs and email addresses
2.  Remove non-letters:
    -   line breaks `\n` and `&nbsp`

    -   punctuation, numbers, and special characters
3.  Add spaces before capital letters then remove extra whitespace
4.  Replace all capital letters with lower case letters

## Example  {.scrollable}

Here's what that looks like for one page.

```{r}
#| echo: true
remove <- c('\n', 
            '[[:punct:]]', 
            'nbsp', 
            '[[:digit:]]', 
            '[[:symbol:]]') %>%
  paste(collapse = '|')

string %>%
  rm_url() %>%
  rm_email() %>%
  str_remove_all('\'') %>%
  str_replace_all(remove, ' ') %>%
  str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
  tolower() %>%
  str_replace_all("\\s+", " ")
```

## Output quality

***Comments:***

-   consistent input format (*i.e.* sampling and collection) is really important for consistent text processing

-   thorough quality tests are recommended: inspect random subsamples for errors in processing

. . .

In this dataset, pages are not consistently formatted. I've done something fast and loose and relatively simple, but it is not perfect.

## Quality checks  {.scrollable}

```{r}
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'s') %>%
    str_replace_all(remove, ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

set.seed(102422)
rawdata_relabeled %>% 
  slice_sample(n = 3) %>% 
  pull(text_tmp) %>%
  lapply(parse_fn)
```

##  {.scrollable}

## Processed data

```{r}
clean <- rawdata_relabeled %>%
  filter(str_detect(text_tmp, '<!')) %>%
  rowwise() %>%
  mutate(text_clean = parse_fn(text_tmp)) %>%
  select(-text_tmp) %>%
  unnest(text_clean)

clean %>% head()
```

## NLP: tokenizing

## NLP: lemmatizing

## NLP: frequency measures
